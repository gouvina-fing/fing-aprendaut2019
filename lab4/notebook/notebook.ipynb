{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 4 - Análisis de datos (PCA, K-Means)\n",
    "\n",
    "### Grupo 07:\n",
    "     - Renzo Gambone C.I. 5.155.486-4\n",
    "     - Germán Ouviña C.I. 4.823.566-1\n",
    "     - Leandro Rodríguez C.I 4.691.736-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "***\n",
    "### 1.1. Objetivo\n",
    "***\n",
    "El objetivo de esta tarea fue analizar bajo distintos enfoques el corpus _aquienvoto.uy_. Más específicamente, se tuvieron en cuenta los siguientes lineamientos:\n",
    "- Implementar un algoritmo de reducción de dimensionalidad (algoritmo PCA) para visualizar los datos.\n",
    "- Implementar un modelo de aprendizaje no supervisado (algoritmo K-means) para agrupar los datos.\n",
    "- Analizar los resultados intermedios generados en cada enfoque, buscando patrones y extrayendo información de utilidad en el proceso.\n",
    "- Analizar los resultados finales generados en cada enfoque, contestando a las preguntas planteadas en el práctico y a otras surgidas de las observaciones del corpus.\n",
    "- Enfocar dichos análisis desde un punto de vista cualitativo, teniendo en cuenta métricas cuantitativas con el fin de orientar dicho análisis.\n",
    "\n",
    "### 1.2. Entrega\n",
    "***\n",
    "La entrega de esta tarea consta de dos grandes componentes:\n",
    "- **Informe** en formato de Jupyter Notebook (este informe).\n",
    "- **Programa** que permite analizar el conjunto de datos utilizando **PCA** o **KMeans**, generar distintas gráficas, evaluar el desempeño de los algoritmos utilizados, entre otras utilidades.\n",
    "\n",
    "El objetivo del **informe** es centralizar la información relativa a los distintos métodos de análisis empleados, así como los resultados obtenidos y las conclusiones generadas. En esta ocasión, es posible ejecutar el código principal completamente desde el informe, agregandose las funciones necesarias en las secciones correspondientes (ver sección 1.3. Para una descripción detallada sobre el código encontrado en cada sección).\n",
    "\n",
    "Por otra parte, el **programa** ofrece una interfaz en consola que permite la ejecución de ambos algoritmos acorde a las distintas configuraciones paramétricas, así como generar todos los datos mostrados en el informe. Si bien todas estas herramientas fueron pensadas para uso del grupo, en el archivo *README.md* se adjunta una sencilla guía de como utilizarlas.\n",
    "\n",
    "A diferencia de las anteriores entregas, el **programa** y su interfaz se agregó como una herramienta puramente auxiliar. Como se mencionó anteriormente, es posible ejecutar el código principal en el **informe**. De todas formas, con el fin de facilitar la lectura, se redujo el uso de código dentro del informe lo máximo posible. Para un mayor entendimiento de los algoritmos empleados, referir a los archivos de código fuente.\n",
    "\n",
    "### 1.3. Estructura\n",
    "***\n",
    "Dado que en esta ocasión el objetivo principal está en el **análisis** y no en la **construcción** de uno o varios modelos, tanto la estructura como las tareas realizadas difieren bastante con respecto a las anteriores entregas. A continuación se adjunta una breve introducción del contenido de cada sección:\n",
    "\n",
    "- En la **Sección 2 (Enfoque)** se define y contrasta los dos grandes enfoques utilizados a lo largo de la tarea (reducción de dimensionalidad y aprendizaje no supervisado), manteniéndose dicha dualidad en el resto de secciones.\n",
    "- En la **Sección 3 (Diseño)** se tratan los aspectos matemáticos y técnicos de cada enfoque, definiendo los algoritmos empleados, formas de evaluación generales, etc.\n",
    "- En la **Sección 4 (Experimentación)** se define la metodología empleada a la hora de experimentar, así como el proceso de generación de resultados, los resultados en sí y el análisis posterior de los mismos. También se desarrolla una sección con información generada durante la investigación previa realizada en relación con el marco teórico del corpus, justificando la toma de decisiones de los distintos experimentos realizados. \n",
    "- En la **Sección 5 (Conclusiones)** se adjuntan las conclusiones generales obtenidas en el desarrollo de la tarea, organizándose las mismas según distintas subsecciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enfoque\n",
    "***\n",
    "Como se mencionó en la sección anterior, el objetivo principal de la tarea es estudiar el corpus de _aquienvoto.uy_. El interés general radica en analizar la relación entre los potenciales votantes, buscando patrones en sus respuestas y enfocándose en el análisis cualitativo, es decir, priorizar las observaciones e interpretaciones de los datos.\n",
    "\n",
    "Existen múltiples estrategias posibles para encarar dicho problema, por lo que a continuación se definen los dos enfoques utilizados:\n",
    "\n",
    "### 2.1. Enfoque 1 - Reducción de Dimensionalidad\n",
    "***\n",
    "Por un lado, es posible analizar los datos sin utilizar ningún modelo de aprendizaje automático. Dado que el corpus cuenta con **26 atributos**, el primer enfoque se centra en la **reducción de dimensionalidad** del corpus en sí, buscando reducir la cantidad de dimensiones para poder visualizar los datos y buscar una relación entre votantes gráficamente.\n",
    "\n",
    "Dicho enfoque puede presentar varios problemas, ya que para poder graficar los datos es necesario pasar de **26 dimensiones** a **2 dimensiones**, siendo factible una pérdida importante de información en el proceso. De hecho, dado un corpus específico, dependiendo de la técnica de reducción de dimensionalidad empleada y de la relación de los datos en sí, es posible determinar que tanto impactará la reducción de dimensiones en la integridad del corpus.\n",
    "\n",
    "De todas formas, si el corpus cuenta con una estructura \"resistente\" a la reducción de dimensiones y el algoritmo utilizado es compatible con dicha estructura, sería posible observar o descartar patrones entre los votantes.\n",
    "\n",
    "Tomando en cuenta lo anterior, existen múltiples técnicas de reducción de dimensionalidad. En el contexto de esta tarea se utilizará **PCA (Principal Component Analysis)**. En la sección 3.1 se especifican los aspectos técnicos de dicho algoritmo y en la sección 4.4 los resultados de la experimentación y análisis empleando dicha técnica.\n",
    "\n",
    "### 2.2. Enfoque 2 - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "Por otra parte, es posible analizar los datos utilizando modelos de aprendizaje automático, más concretamente modelos que implementen técnicas de **clustering** o **agrupamiento**, siendo está una forma de **aprendizaje no supervisado**.\n",
    "\n",
    "Dicho enfoque genera distintos grupos de votantes, basándose en su distribución en un espacio de **26 dimensiones**. Dependiendo de la naturaleza del corpus, esto puede generar problemas. Al igual que en el anterior enfoque, es posible determinar la cantidad óptima de grupos a generar (ya que a priori, esta es desconocida).\n",
    "\n",
    "Tomando en cuenta lo anterior, existen múltiples técnicas de clustering. En el contexto de esta tarea se utilizará **K-Means**. En la sección 3.2 se especifican los aspectos técnicos de dicho algoritmo y en la sección 4.5 los resultados de la experimentación y análisis empleando dicha técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diseño\n",
    "***\n",
    "En esta sección se detallan las características del diseño utilizado para construir las herramientas de análisis para cada enfoque, profundizándose las estrategias y algoritmos empleados, así como otros puntos, como el procesamiento previo al análisis y/o entrenamiento y la evaluación posterior al mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Enfoque 1 - Reducción de Dimensionalidad\n",
    "***\n",
    "\n",
    "#### 3.1.1. Algoritmo\n",
    "***\n",
    "Como se mencionó en la sección anterior, el algoritmo empleado para reducir las dimensiones del corpus fue **PCA**. Como su nombre indica, el algoritmo hace uso de los **componentes principales** de una matriz. Se define como componente principal de una matriz al mayor **valor propio** de la misma.\n",
    "\n",
    "En términos generales, el algoritmo consiste en calcular $k$ componentes principales de cierta matriz, formar otra matriz con vectores propios correspondientes a cada componente principal y transformar el subespacio del corpus multiplicando cada vector del mismo por la matriz de vectores propios. Es importante destacar que $k$ es la cantidad de dimensiones que se quiere mantener, teniendo que ser entero positivo menor a la cantidad de dimensiones actual.\n",
    "\n",
    "Dado que existen múltiples métodos para implementar **PCA**, la matriz que se obtiene y por ende los componentes principales de la misma dependen del método. No obstante, los resultados son ortogonales al método, es decir, la reducción de dimensionalidad obtenida es independiente al método utilizado.\n",
    "\n",
    "Teniendo esto en cuenta, con el objetivo de validar métodos se implementaron dos formas distintas de **PCA**:\n",
    "\n",
    "##### 3.1.1.1. Matriz de Covarianza\n",
    "***\n",
    "\n",
    "Sea un vector de atributos $A = [A_1, ..., A_n]$ y sea $\\mu_i$ la media del atributo $A_i$, se define la **covarianza** entre dos atributos $A_i, A_j$ como la varianza conjunta de ambos atributos, utilizando la siguiente expresión:\n",
    "\n",
    "$$ Cov(A_i, A_j) = E((A_i - \\mu_i)(A_j - \\mu_j)) $$\n",
    "\n",
    "En cierta forma, la **covarianza** entre $A_i$ y $A_j$ representa la relación entre la varianza de ambos atributos por separado. Una **covarianza positiva** indica que ambos atributos crecen de forma _directamente proporcional_, mientras que una **covarianza negativa** indica que ambos atributos crecen de forma _inversamente proporcional_.\n",
    "\n",
    "De esta forma, la **matriz de covarianza** $M$ de un conjunto de datos $D$ con un vector de atributos $A$ se define como la covarianza de cada posible par de atributos de $A$, utilizando la siguiente expresión:\n",
    "\n",
    "$$M(D,A) = (Cov(A_i,A_j))_{ij} : 1 \\leq i, j \\leq |A| $$\n",
    "\n",
    "Como se mencionó anteriormente, **PCA** utiliza los componentes principales de una matriz para reducir la dimensión del subespacio que el corpus representa. En este método, la matriz empleada es nada más y nada menos que la **matriz de covarianza** para el corpus $D$ y sus atributos $A$.\n",
    "\n",
    "Teniendo esto en cuenta, el algoritmo implementado consiste en los siguientes pasos:\n",
    "1. Sustraer la media $\\mu_i$ para cada atributo $A_i$ del conjunto de datos (esto transforma al corpus en un conjunto de datos con media 0 para cada atributo).\n",
    "2. Calcular la **matriz de covarianza** $M(D,A)$.\n",
    "3. Calcular los **valores propios** de $M(D,A)$ y obtener un **vector propio** para cada valor.\n",
    "4. Elegir los $k$ **componentes principales** de $M(D,A)$ y formar la matriz $W$ con los $k$ vectores propios correspondientes a los valores propios elegidos (y en el orden adecuado).\n",
    "5. Sea $D$ la matriz que representa al corpus original, se calcula $T = DW$, teniendo esta $k$ dimensiones.\n",
    "\n",
    "\n",
    "##### 3.1.1.2. Descomposición SVD\n",
    "***\n",
    "\n",
    "Otro potencial método para obtener una matriz de la cual extraer los componentes principales es la **descomposición SVD** o **descomposición en valores singulares**. \n",
    "\n",
    "Sea $M \\in R_{m \\times n}$ una matriz, su descomposición SVD se define como una factorización del tipo $M = USV$ donde $U,S,V$ son matrices que cumplen las siguientes propiedades:\n",
    "- La matriz $U \\in R_{m \\times m}$ está formada por vectores propios de $M . M^t$, siendo $U$ ortonormal.\n",
    "- La matriz $V \\in R_{n \\times n}$ está formada por vectores propios de $M^t . M$, siendo $V$ ortonormal.\n",
    "- La matriz $S \\in R_{m \\times n}$ está formada por los **valores singulares** de $M$ en su diagonal principal ordenados de mayor a menor.\n",
    "\n",
    "Cabe destacar que los **valores singulares** de una matriz son las raíces cuadradas de sus **valores propios** por lo que el mayor valor singular de una matriz es la raíz cuadrada del componente principal de dicha matriz.\n",
    "\n",
    "El método implementado se sirvió de obtener los valores propios de la matriz de covarianza, utilizando luego aritmética de matrices para obtener una matriz $T$ que represente al corpus $D$ con dimensionalidad reducida.\n",
    "\n",
    "Dado que el método con **SVD** no aporta más que una validación a la hora de comparar con el método anterior, no se entrará en detalles de su implementación. Para ver dicha implementación, referir al código fuente del módulo _pca.py_.\n",
    "\n",
    "#### 3.1.2. Evaluación\n",
    "***\n",
    "\n",
    "Los algoritmos de reducción de dimensionalidad tienen distinto rendimiento dependiendo de su forma de operar y del conjunto de datos sobre el que trabajen. En cualquier caso, cuanto más se reducen las dimensiones de un corpus, más información se pierde.\n",
    "\n",
    "Normalmente, al momento de reducir las dimensiones de un conjunto de datos utilizando **PCA**, surge la necesidad de decidir cuántas componentes se van a considerar para que no se pierda el significado que contienen los datos en sí. Para esto se suele calcular una métrica denominada **ratio de varianza explicada**, la cual permite determinar en términos generales la pérdida de información provocada al reducir las dimensiones para cada posible $k$ que se tome. De esta forma, dependiendo de donde se ponga el límite, el menor $k$ que supere ese límite sería la cantidad \"ideal\" de componentes a tomar.\n",
    "\n",
    "Para calcular el **ratio de varianza explicada** se procesan los valores propios, calculándose la suma acumulativa para cada cantidad de componentes consideradas en cada caso. De esta forma, para cierto $k$, se obtiene que esta métrica representa la suma de los $k$ componentes principales sobre la suma de todos los valores propios. Cuanto mayor es este valor, mejor es esa elección de $k$. Lógicamente, cuanto mayor es $k$, mayor es el ratio de varianza explicada (de hecho, es creciente).\n",
    "\n",
    "En este contexto en particular, se sabe que el interés de utilizar **PCA** es reducir a **2 dimensiones**, fijando $k = 2$. No obstante, utilizando el **ratio de varianza explicada**, se puede conocer que tanta información se está perdiendo. En la sección de experimentación 4.4 se muestran los resultados de la evaluación de **PCA** y sus implicancias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Enfoque 2 - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "\n",
    "#### 3.2.1. Modelo\n",
    "***\n",
    "Las técnicas de **aprendizaje no supervisado** difieren sustancialmente de las técnicas implementadas en anteriores entregas, las cuales son del grupo de técnicas de **aprendizaje supervisado**. En esencia, el aprendizaje supervisado utiliza para entrenar un conjunto de datos con clasificación conocida, mientras que el aprendizaje no supervisado, en lugar de vincular elementos del conjunto con su clasificación, se concentra en observar patrones e irregularidades en la distribución de los elementos en el espacio comprendido por sus atributos.\n",
    "\n",
    "Dentro de las técnicas de aprendizaje no supervisado, existe una que se denomina **clustering**, la cual tiene como objetivo agrupar los elementos del conjunto de datos en subconjuntos disjuntos denominados **clusters**, los cuales agrupan elementos similares de alguna forma en cuanto a sus atributos, siguiendo distintas nociones dependiendo del modelo implementado.\n",
    "\n",
    "Cabe mencionar que los resultados obtenidos no necesariamente reflejan conclusiones que sean de valor para un clasificador, sino que los mismos están orientados netamente hacia la distribución de los elementos en el conjunto de atributos. Observar como los elementos están distribuidos no es trivial para un corpus con alta dimensionalidad, pero permite destacar anomalías en el conjunto y agrupar ejemplos \"similares\". Por otra parte, es posible que la agrupación en clusters esté vinculada con la clasificación de los elementos de un cluster, sin embargo esta presunción no puede realizarse sin la experimentación adecuada.\n",
    "\n",
    "Teniendo en cuenta las generalidades anteriormente descritas, en el contexto de esta tarea se implementó el modelo de **clustering** denominado **K-Means Clustering** (a partir de ahora denominado **K-Means**), el cual representa el conjunto de datos como un espacio de **$n$ dimensiones** (siendo $n$ la cantidad de atributos) e itera sobre el mismo, buscando determinar los anteriormente mencionados **clusters**.\n",
    "\n",
    "Se definen más formalmente las siguientes nociones:\n",
    "* $D =$ Conjunto de entrenamiento.\n",
    "* $C_D =$ Conjunto de posibles clasificaciones para $d \\in D$.\n",
    "* $KMeans_{(D,k)} =$ Agrupación del conjunto $D$ en $k$ clusters, los cuales son representados por sus **centroides**, los cuales son coordenadas en el espacio de los atributos que determinan la posición espacial de cada cluster.\n",
    "* Para un ejemplo $d \\in D$, es posible determinar a que cluster pertenece asociándolo al cluster cuyo centroide esté más próximo a el mismo.\n",
    "\n",
    "Los detalles sobre el algoritmo de entrenamiento y de agrupación de nuevos ejemplos se expanden en la siguiente sección.\n",
    "\n",
    "#### 3.2.2. Algoritmo\n",
    "***\n",
    "\n",
    "##### 3.2.2.1. Algoritmo de entrenamiento\n",
    "***\n",
    "El algoritmo **K-Means** implementado sigue los lineamientos del visto en el teórico, por lo que no es necesario aclarar ninguna particularidad de su implementación, más que el hecho de que el umbral de variación entre los centroides obtenidos en una iteración y el siguiente debe ser de 0.0001 o menor para que el algoritmo considere haber terminado.\n",
    "\n",
    "##### 3.2.2.2. Algoritmo de agrupación\n",
    "***\n",
    "Con el objetivo de separar conceptualmente de la clasificación vista en modelos de aprendizaje supervisado, se le denomina **agrupación** a determinar el cluster al que debería pertenecer un nuevo ejemplo que no se encontraba en el conjunto de datos original.\n",
    "\n",
    "Dada la naturaleza de **K-Means**, el algoritmo de agrupación es posible y ligero en términos de computo. Consiste en simplemente calcular la distancia euclídea del ejemplo a agrupar para cada uno de los centroides de cada cluster. Aquel centroide que se encuentre más cerca del ejemplo, representará al cluster al que dicho ejemplo pertenece.\n",
    "\n",
    "Es crucial mencionar que nuevos ejemplos podría afectar a los clusters generados originalmente, cambiando la distribución de ejemplos e incluso la ubicación de los centroides. Por ende, para ciertos escenarios es incorrecto utilizar este enfoque de agrupación de nuevos ejemplos y debe correrse el algoritmo de entrenamiento otra vez. Dado que la implementación para esta tarea no tiene en cuenta la agrupación de nuevos ejemplos en ningún punto, se consideró un enfoque posible. \n",
    "\n",
    "#### 3.2.4. Evaluación\n",
    "***\n",
    "Debido a que el aprendizaje no supervisado no apela a generar un clasificador y validarlo con un subconjunto de prueba, no es posible evaluar su rendimiento utilizando las métricas de tareas anteriores. De hecho, a priori no existe una clasificación \"correcta\".\n",
    "\n",
    "Teniendo esto en cuenta, se evaluaron los potenciales problemas del modelo **K-Means** y se planteó el uso de distintas métricas para determinar que clusters agrupan mejor a los datos. A continuación se especifican dichos problemas:\n",
    "\n",
    "##### 3.2.4.1. Problemas del modelo\n",
    "***\n",
    "\n",
    "**K-Means** tiene dos problemas principales:\n",
    "\n",
    "* La función de costo a minimizar no es convexa, y por ende, tiene óptimos locales. A su vez el algoritmo **K-Means** tiene solamente componentes de *explotación* y no de *exploración*, por lo que es propenso a quedarse atrapado en el primer óptimo local que encuentre.\n",
    "\n",
    "* La elección del número $k$ no es trivial para cada conjunto de datos. Un valor de $k$ bajo será más eficiente computacionalmente y tendrá una función de costo más fácil de optimizar, pero las agrupaciones en clusters no necesariamente reflejarán una clara distribución de los elementos en el conjunto. Por otra parte, un valor de $k$ alto puede tener una mayor cantidad de óptimos locales y puede subdividir el conjunto en más clusters de lo que realmente tiene sentido para la distribución del conjunto.\n",
    "\n",
    "Sabiendo esto, en la experimentación se decidió implementar ciertas estrategias con el objetivo de palear la influencia de estos problemas, siendo las mismas:\n",
    "\n",
    "* Realizar $n$ ejecuciones independientes del algoritmo (siendo $n$ un parámetro de entrada), manteniendo los resultados de mejor calidad, acorde a cierta métrica (la cual se define en la siguiente subsección). De esta forma se busca dar un componente de *exploración* al algoritmo.\n",
    "\n",
    "* Realizar ejecuciones para distintos valores de $k$, estudiando su comportamiento en relación a la variación de $k$.\n",
    "\n",
    "En la sección de experimentación 4.5 se profundiza sobre los enfoques tomados y los resultados obtenidos gracias a ello.\n",
    "\n",
    "##### 3.2.4.2. Métricas empleadas\n",
    "***\n",
    "\n",
    "Para cuantificar la calidad del agrupamiento en clusters del algoritmo **K-Means** se emplearon dos métricas distintas que ofrecen distintas nociones sobre la calidad de los clusters elegidos. Las implementaciones de dichas métricas son las brindadas por la biblioteca *sklearn* de Python.\n",
    "\n",
    "Es importante mencionar que, de cierta forma, estas métricas atentan contra la idea del aprendizaje no supervisado, ya que se utilizan clasificaciones conocidas para evaluar la calidad de los clusters. Dichas clasificaciones no siempre están disponibles y a veces ni siquiera existen. Sin embargo, en este contexto se cuenta con esos datos, por lo cual se utilizarán para encontrar patrones.\n",
    "\n",
    "Las métricas son las siguientes:\n",
    "\n",
    "###### 3.2.4.2.1. Silhouette\n",
    "***\n",
    "El **coeficiente de Silhouette** evalúa los resultados del modelo observando la densidad de cada cluster según la clasificación original de cada elemento. Un coeficiente alto se relaciona con que los resultados tienen clusters \"bien definidos\".\n",
    "\n",
    "Dados los resultados:\n",
    "* **a:** La distancia promedio entre una muestra y todos los otros puntos de la misma clase.\n",
    "* **b:** La distancia promedio entre una muestra y todos los otros puntos en el cluster más próximo.\n",
    "\n",
    "Se define el **coeficiente de Silhouette** como: $$s = \\frac{b-a}{max(a,b)}$$\n",
    "\n",
    "Para un conjunto de muestras, se toma el promedio general del **coeficiente de Silhouette** para cada muestra.\n",
    "\n",
    "###### 3.2.4.2.2. Adjusted Random Index (ARI)\n",
    "***\n",
    "La métrica **ARI** se encuentra comprendida en el intervalo $[-1..1]$, y dadas dos listas con valores \"reales\" y \"predecidos\" mide la similaridad entre los elementos de las mismas, ignorando posibles permutaciones. En esencia, esta métrica es útil para comprobar si efectivamente cada cluster está vinculado con las clasificaciones originales del conjunto o con otros posibles agrupamientos conocidos.\n",
    "\n",
    "A diferencia del *coeficiente de Silhouette*, esta métrica sólo tiene sentido para determinados valores de $k$ (valores a los cuales se pueda asociar una clasificación), dado que no evalúa según la densidad de cada cluster, sino según como el conjunto entero fue agrupado en clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimentación\n",
    "***\n",
    "En esta sección se detallan los aspectos relacionados a las distintas pruebas realizadas, tratando tanto los lineamientos previos como los análisis realizados sobre los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Metodología\n",
    "***\n",
    "\n",
    "Dado el objetivo de buscar patrones en el corpus, se utilizó una metodología de experimentación particular, dividiéndola en las siguientes etapas:\n",
    "\n",
    "1. **Investigación previa:** Al tratarse de un corpus con un contexto específico (posibles votaciones en elecciones nacionales), fue necesario realizar una investigación genérica sobre dicho contexto para poder evaluarlo lo mejor posible. En resumen, se investigó sobre generalidades de la política partidaria, análisis estadísticos en poblaciones de votantes, variables indicadoras, entre otros. En la sección 4.2 se profundiza la información recabada y los sesgos introducidos por la interpretación de la misma.\n",
    "<br><br>\n",
    "2. **Análisis del corpus:** Habiendo realizado el sondeo de información básica sobre el tema, se analizó la estructura del corpus, especulando sobre su correctitud. En la sección 4.3 se estudian las posibles ventajas y desventajas de esta estructura.\n",
    "<br><br>\n",
    "3. **Generación de resultados:** Una vez realizada la investigación y el análisis preliminar, se ejecutaron distintas configuraciones de ambos enfoques con el fin de observar los datos resultantes. En la sección 4.4 se expanden las elecciones de configuraciones utilizadas, la justificación de dichas elecciones, los resultados más importantes y se adjuntan los scripts que fueron implementados.\n",
    "<br><br>\n",
    "4. **Análisis de enfoques:** Teniendo los resultados generados, se realizó un estudio de los mismos con un enfoque de análisis cualitativo, utilizando los conocimientos generados en las etapas previas. Se analizaron métricas descriptivas de ambos algoritmos, también realizándose observaciones e interpretaciones de cada resultado. En la sección 4.5 se expanden estos puntos y se adjuntan los scripts necesarios para evaluar el desempeño del algoritmo utilizado y la calidad de los resultados analizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Marco Teórico\n",
    "***\n",
    "\n",
    "Dada la naturaleza del corpus, fue necesario emplear cierta cantidad de tiempo en realizar una investigación previa, con el objetivo de contextualizar el análisis. Por una parte, fue necesario investigar sobre generalidades de **política**, como el concepto de **partido político** y distintas formas de clasificar posturas dentro de diversos **espectros políticos**. Una vez se hubieron bajado a tierra dichos conceptos, fue necesario aplicarlos al contexto nacional y contemporáneo, investigando las propuestas de cada candidato.\n",
    "\n",
    "Dado que no es el objetivo de esta tarea, no se entrará en detalles de los conceptos teóricos estudiados. En la sección 6 (Referencias) se adjuntan los textos utilizados en la investigación.\n",
    "\n",
    "De esta forma, viendo a cada ejemplo del corpus como un **votante**, se determinaron 4 posibles agrupamientos de los mismos, con el fin de estudiar su distribución en el corpus y su relación con los atributos. De estos 4 grupos, los 2 primeros son **triviales** (no existe dos formas de interpretarlos), mientras que los últimos 2 son **sesgados** (se pueden interpretar de varias formas). Dichos grupos se generaron en base a los siguientes lineamientos:\n",
    "\n",
    "1. **Candidatos:**\n",
    "    \n",
    "    El agrupamiento predefinido, donde existe un grupo por candidato y cada grupo contiene a los votantes de dicho candidato. Existen **24 candidatos**.\n",
    "\n",
    "\n",
    "2. **Partidos:** \n",
    "    \n",
    "    Otro agrupamiento trivial, donde existe un grupo por partido político y cada grupo contiene a los votantes de aquellos candidatos que pertenecen al partido. Dado que cada candidato pertenece a un único partido, no existe ambigüedad en la asignación de partidos a candidatos. Existen **11 partidos**.\n",
    "\n",
    "\n",
    "3. **Espectro unidimensional:**\n",
    "\n",
    "    El primero de los agrupamientos sesgados, denominándose así ya que la asignación de candidatos no es trivial ni única, pudiendo intrepretarse de varias formas. Este agrupamiento se fundamenta en los principios denominados **izquierda** y **derecha**, originados durante la revolución francesa y mantenidos a lo largo del tiempo. Si bien en ciertos ámbitos de la política se pueden considerar obsoletos, ambos grupos representan dos \"extremos\" de un espectro político unidimensional. \n",
    "\n",
    "    Originalmente, se consideraba de **derecha** a aquellos individuos que defendían el status qúo, mientras que los individuos de **izquierda** buscaban el cambio, entendiendo al status qúo como injusto y retrógrado. Con el paso del tiempo y el cambio rotundo en la organización de muchas sociedades del mundo, se empezó a considerar a la **derecha** como aquella ideología que defendía la existencia de las clases sociales y la desigualdad social, mientras que la **izquierda** es la ideología que se opone a dicha noción. Si bien los intereses que ambos grupos defienden varían dependiendo del contexto histórico y regional, la utilización de ambos términos para agrupar personas según esta ideología política prevalece, ya que a grandes rasgos captan la esencia de polos opuestos.\n",
    "\n",
    "    Teniendo esto en cuenta, es importante marcar que si bien se oponen, existe un **espectro** de por medio, denominándose **extrema izquierda** y **extrema derecha** respectivamente a aquellas ideologías que no toman ninguna característica de la opuesta. Una postura que combina características de ambos extremos \"en partes iguales\" se denomina **central**, mientras que las tendencias del centro a cada extremo son denominadas **centroizquierda** y **centroderecha** respectivamente.\n",
    "    \n",
    "    De esta forma, y luego de investigar las posturas de cada candidato y partido ante estas clasificaciones, se determinaron **3 grupos**: izquierda, centro y derecha. A su vez, se clasificó a cada candidato dentro de cada uno de los grupos. Dicha clasificación puede ser incorrecta, por lo que los resultados se tomarán con pinzas.\n",
    "\n",
    "\n",
    "4. **Espectro bidimensional:**\n",
    "\n",
    "    Este agrupamiento sesgado también se fundamenta en los principios denominados **izquierda** y **derecha**, pero complejiza el espectro agregando una segunda dimensión y modificando la idea de la primera. Dichas dimensiones se denominan **libertad personal** y **libertad económica**, la primera haciendo referencia a que aquellas políticas sociales que dan libertad a los individuos en su vida diaria implican una mayor libertad personal, mientras que la segunda hace referencia a que aquellas políticas económicas que dan libertad a individuos o empresas para operar con poco control del Estado implican una mayor libertad económica.\n",
    "       \n",
    "    Ambas nociones fueron acuñadas por _David Nolan_, quien inventó la conocida **gráfica de Nolan** que utiliza ambas nociones para clasificar en un espectro bidimensional a candidatos o partidos según la relación de sus políticas con los conceptos de libertad personal y económica.\n",
    "\n",
    "    <div style=\"display: inline-block; width: 99%; text-align: center; margin-top: 16px;\">\n",
    "        <img src=\"img/nolan.png\" />\n",
    "        <label style=\"margin-top: 16px; font-size: 16px; font-family: monospace;\"> Figura 4.2 - Gráfica de Nolan</label>\n",
    "    </div>\n",
    "    \n",
    "    En la gráfica se definen los 4 extremos como **progresismo**, **liberalismo**, **conservadurismo**, **totalitarismo**. Tomando el espectro unidimensional, el progresismo es asociado con la izquierda (libertad personal alta pero control económico por parte del Estado), mientras que el conservadurismo es asociado con la derecha (privatización de empresas y poco control del Estado en materias económicas, pero fuerte imposición de normas sociales). Por otra parte, el totalitarismo es asociado con el control total por parte del Estado en la población, mientras que el liberalismo sitúa al Estado como un mero actor con mucha menos influencia.\n",
    "    \n",
    "    De esta forma, y luego de investigar las posturas de cada candidato y partido ante estas clasificaciones, se determinaron **5 grupos**: progresismo, liberalismo, conservadurismo, totalitarismo y centro. A su vez, se clasificó a cada candidato dentro de cada uno de los grupos. Dicha clasificación fue sumamente tendenciosa dada la falta de información y lo complejo de establecer un límite en una clasificación espectral, por lo que es muy potencialmente errónea. No obstante, se decidió mantenerla con el fin de identificar algún posible patrón.\n",
    "\n",
    "Es importante volver a destacar que dichas separaciones se encuentran sumamente sesgadas, utilizándose simplemente para obtener una noción de como se distribuye la información en el corpus y en los clusters generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Análisis - Corpus\n",
    "***\n",
    "\n",
    "Habiendo generado una base para poder analizar el corpus y determinar su correctitud cualitativamente, se realizaron las siguientes observaciones sobre el **conjunto de atributos**:\n",
    "- Hay 3 grupos grandes de preguntas: **Economía**, **Seguridad** y **Sociedad**.\n",
    "- Hay 26 preguntas en total, las cuales perteneces a alguno de los 3 grupos anteriores.\n",
    "- Hay 5 posibles puntajes, del 1 al 5, para cada pregunta. Cuánto mayor puntaje, mayor acuerdo se expresa con la pregunta.\n",
    "\n",
    "Tomando en cuenta dichas observaciones, se realizaron ciertas interpretaciones:\n",
    "- Las categorías **economía** y **sociedad** agrupan dos de los grandes conceptos tratados en la **gráfica de Nolan**, sin embargo, la noción de **seguridad** no entra en dicho apartado. Bajo cierta interpretación, podría considerarse parte de lo que es sociedad. Dado que es un tema recurrente en los medios uruguayos desde hace algunos años, puede tener sentido que exista un conjunto apartado de preguntas sobre este tema, mas también puede considerarse tendencioso y por ende puede estar agregando ruido.\n",
    "- Las preguntas tienen un carácter bastante general y asignan una representación numérica al acuerdo con cada una. Dada la complejidad de los conceptos que se manejan en algunas preguntas, es posible que dicha representación de respuestas también genere ruido.\n",
    "\n",
    "De todas formas, dado lo complejo que puede llegar a ser el análisis matemático de variables sociales, es entendible que se hayan elegido ciertas representaciones, teniendo en cuenta el posible alejamiento de la realidad.\n",
    "\n",
    "En lo que respecta al **conjunto de datos**, dicho análisis se deja para las siguientes secciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Análisis - Reducción de dimensionalidad\n",
    "***\n",
    "\n",
    "#### 4.4.1. Proceso\n",
    "***\n",
    "Como se mencionó en anteriores secciones, el objetivo de reducir la dimensionalidad del corpus a 2 dimesiones es observar su comportamiento gráficamente y determinar posibles patrones.\n",
    "\n",
    "Con este conocimiento, se encaró la experimentación de la siguiente manera:\n",
    "1. Se ejecutó **PCA** sobre el corpus, generando un corpus de dimensión reducida a 2.\n",
    "2. Se generaron gráficas de dicho corpus, marcando ejemplos según las divisiones establecidas en la sección 4.2, con el objetivo de analizar cualitativamente la distribución de datos.\n",
    "3. Se generaron gráficas de datos intermedios generados por la ejecución de **PCA**, con el objetivo de evaluar el rendimiento del algoritmo ejecutado y su eficacia en el corpus dado.\n",
    "\n",
    "A continuación, se desglosan las tres etapas.\n",
    "\n",
    "#### 4.4.2. Resultados\n",
    "***\n",
    "A continuación se adjunta un script que ejecuta **PCA** sobre el corpus, graficando en dos dimensiones y almacenado globalmente los resultados. Es **necesario** ejecutarlo antes de ejecutar cualquier script de las siguientes subsecciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'reducedDataset' (ndarray)\n",
      "Stored 'extras' (dict)\n",
      "Stored 'candidates' (Series)\n",
      "Stored 'defaultOptionsPCA' (dict)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7cb0256400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Script PCA - Ejecutar antes que cualquier gráfica de PCA\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from utils.const import *\n",
    "from model import pca\n",
    "import processing.reader as reader\n",
    "import plotting.pcaPlotting as pcaPlotting\n",
    "\n",
    "# Leer dataset de respuestas a encuesta\n",
    "candidates, dataset = reader.readDataset('../' + DATA_ENCUESTAS)\n",
    "\n",
    "# Almacenar opciones por defecto para graficar luego\n",
    "defaultOptionsPCA = {\n",
    "    'pca_election': PCAOps.COVARIANZA,\n",
    "    'pca_analysis': PCAnalysis.GENERAL,\n",
    "    'pca_intermediates': PCAIntermediates.NONE,\n",
    "    'candidate_division': CandidateDivision.PARTIES,\n",
    "    'from_notebook': True\n",
    "}\n",
    "\n",
    "# Aplicar PCA para reducir a 2 dimensiones\n",
    "reducedDataset, extras = pca.reduce_pca(dataset.values, 2, defaultOptionsPCA)\n",
    "\n",
    "# Almacenamiento de variables globales para graficar luego\n",
    "%store reducedDataset\n",
    "%store extras\n",
    "%store candidates\n",
    "%store defaultOptionsPCA\n",
    "\n",
    "# Graficar corpus en 2 dimensiones\n",
    "pcaPlotting.plotPCA(reducedDataset, candidates, defaultOptionsPCA, extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la gráfica obtenida, se puede observar que los ejemplos se distribuyen en el intervalo $[-10,10] \\subset \\mathbb{R}$ en ambos ejes, en contraste con el corpus original de 26 dimensiones, del cual se sabe que las coordenadas de cada ejemplo se distribuyen en el intervalo $[1,5] \\subset \\mathbb{N}$.\n",
    "\n",
    "A priori este conocimiento no indica nada trivial, por lo que se prosiguió a realizar un análisis cualitativo de estos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3. Análisis cualitativo\n",
    "***\n",
    "En la siguiente sección se adjuntan diversos scripts para la muestra de gráficas utilizando el corpus reducido por **PCA** en la sección anterior. Más concretamente, se grafican los resultados etiquetados según las divisiones establecidas en la sección 4.2 (votantes según partido votado, votantes según posición en espectro izquierda, derecha, centro y votantes según posición en espectro progresismo, liberalismo, conservadurismo, centro).\n",
    "\n",
    "Es importante recordar dos cosas:\n",
    "- La separación de candidatos para el espectro político unidimensional y bidimensional se encuentra determinada en los archivos _data/candidatosEspectro.json_ y  _data/candidatosNolan.json_ respectivamente.\n",
    "- Dicha separación es arbitraria y si bien se presume no del todo correcta (especialmente para el espectro bidimensional), se mantuvo con el fin de expandir las evaluaciones posibles.\n",
    "\n",
    "Teniendo esto en cuenta, se adjunta un script por cada división:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica PCA - División por partido político\n",
    "options = defaultOptionsPCA.copy()\n",
    "options['pca_analysis'] = PCAnalysis.ALL_PARTY\n",
    "options['candidate_division'] = CandidateDivision.PARTIES\n",
    "pcaPlotting.plotPCA(reducedDataset, candidates, options, extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica PCA - División por espectro político unidimensional (izquierda, derecha, centro)\n",
    "options = defaultOptionsPCA.copy()\n",
    "options['pca_analysis'] = PCAnalysis.ALL_PARTY\n",
    "options['candidate_division'] = CandidateDivision.SPECTRUM\n",
    "pcaPlotting.plotPCA(reducedDataset, candidates, options, extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica PCA - División por espectro político bidimensional (progresismo, liberalismo, conservadurismo, centro)\n",
    "options = defaultOptionsPCA.copy()\n",
    "options['pca_analysis'] = PCAnalysis.ALL_PARTY\n",
    "options['candidate_division'] = CandidateDivision.NOLAN\n",
    "pcaPlotting.plotPCA(reducedDataset, candidates, options, extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independientemente a la gráfica que se observe, no se puede encontrar ningún patrón discernible en la distribución de los datos, por lo que existen dos posibles conclusiones:\n",
    "- No existe un patrón discernible por el ojo humano en el corpus de datos.\n",
    "- La reducción de dimensionalidad en el corpus original provoca una pérdida de información demasiado importante como para mantener los patrones del corpus original.\n",
    "\n",
    "Es importante destacar que si bien ambas conclusiones son opuestas, es posible que la realidad posea parte de ambas, es decir, que el patrón en el conjunto original sea difícil de discernir y a su vez la pérdida de información sea crucial.\n",
    "\n",
    "En la siguiente sección se evalúa la idoneidad del corpus para la ejecución de **PCA**, es decir, que tanto impacto tiene la reducción de dimensionalidad en el conjunto original y si esto está relacionado con la imposibilidad de encontrar un patrón en el corpus reducido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4. Evaluación\n",
    "***\n",
    "Primeramente, resulta de interés observar el comportamiento de los **valores propios** obtenidos de la **matriz de covarianza**. Dichos valores propios aportan información interesante del corpus original. Se destacan ciertas nociones de álgebra lineal y su relación con **PCA**:\n",
    "- Si todos los valores propios tienen magnitud muy similar, es un indicador de que el dataset ya está en un \"buen\" subespacio, en el sentido de proporción de información aportada por cada dimensión. Sin embargo, esto también significa que reducir dimensiones reducirá la información con un orden lineal, dado que cada vector propio aporta información por igual al conjunto.\n",
    "- Si algunos valores propios tienen un orden de magnitud mayor al resto, estos son buenos candidatos para conservar en PCA.\n",
    "- Si algunos valores propios tienen un valor cercano a 0, estos son buenos candidatos para descartar en PCA, dado que la ganancia de información de los mismos es casi nula.\n",
    "\n",
    "Teniendo esto en cuenta, se adjunta a continuación un script que contrasta los valores de los **26 valores propios** de la **matriz de covarianza** calculada en **PCA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica PCA (Resultados Intermedios) - Distribución de Valores Propios\n",
    "options = defaultOptionsPCA.copy()\n",
    "options['pca_analysis'] = PCAnalysis.NONE\n",
    "options['pca_intermediates'] = PCAIntermediates.EIGEN_VALUES\n",
    "pcaPlotting.plotPCA(reducedDataset, candidates, options, extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los resultados y teniendo en cuenta las nociones mencionadas, se puede observar lo siguiente:\n",
    "- El **componente principal** de la matriz es un valor propio cuyo orden de magnitud es considerablemente mayor al resto.\n",
    "- El resto de los **valores propios** tiene un orden de magnitud similar entre sí.\n",
    "- El **componente principal** (19) no es descomunalmente grande en comparación al resto de valores. Si bien la información que su vector contiene es ciertamente mayor, la suma del resto de los valores propios es de **35.12**, por lo que descartarlos conlleva a una considerable perdida de información.\n",
    "\n",
    "Teniendo en cuenta dichas observaciones, se interpreta lo siguiente:\n",
    "- El conjunto original en sus **26 dimensiones** no necesariamente se encuentra en un \"buen\" espacio, dada la diferencia de magnitud con el **componente principal**.\n",
    "- A partir de una dimensión, si bien la perdida de información al agregar más dimensiones no es lineal, el orden de magnitud entre el resto de los **valores propios** es suficientemente similar como para que tomar sólo dos dimensiones siga implicando una gran perdida de información.\n",
    "\n",
    "En resumen, se cuenta con la intuición de que la reducción a 2 dimensiones no engloba una suficiente proporción de la información original como para mantener los patrones. Para bajar más a tierra dicha noción se adjunta a continuación un script que calcula el **ratio de varianza explicado** para cada posible cantidad de componentes a tomar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica PCA (Resultados Intermedios) - Ratio de Varianza Explicado\n",
    "options = defaultOptionsPCA.copy()\n",
    "options['pca_analysis'] = PCAnalysis.NONE\n",
    "options['pca_intermediates'] = PCAIntermediates.VARIANCE_RATIO\n",
    "pcaPlotting.plotPCA(reducedDataset, candidates, options, extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos resultados respaldan las interpretaciones realizadas en la sección anterior, y a su vez, permiten generar otras observaciones interesantes:\n",
    "- Efectivamente, al tomar sólo dos dimensiones se pierde un **59.7%** de la información original.\n",
    "- Debido a que la curva crece de forma similar a medida que crece la cantidad de componentes, es complejo elegir una cantidad de componentes idónea para reducir la dimensionalidad.\n",
    "- No obstante, se determinaron ciertos candidatos, como por ejemplo:\n",
    "    - $k = 8$, con un **34.8%** de perdida de información.\n",
    "    - $k = 14$, con un **20.1%** de perdida de información.\n",
    "- Sin embargo ninguna de estas dimensiones tiene una clara representación gráfica y tampoco se puede garantizar que formen subconjuntos claramente definidos para cada clasificación.\n",
    "    \n",
    "En resumen, el **ratio de varianza explicado** para 2 dimensiones parece confirmar el hecho de que la pérdida de información es demasiada como para encontrar patrones gráficamente. No obstante, no permite asegurar ni negar la existencia de dichos patrones en el corpus original.\n",
    "\n",
    "Con el objetivo de estudiar el corpus original, se adjunta a continuación un script que muestra la **matriz de covarianza** como un mapa de calor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica PCA (Resultados Intermedios) - Matriz de Covarianza (Mapa de Calor)\n",
    "options = defaultOptionsPCA.copy()\n",
    "options['pca_analysis'] = PCAnalysis.NONE\n",
    "options['pca_intermediates'] = PCAIntermediates.COV_MATRIX\n",
    "pcaPlotting.plotPCA(reducedDataset, candidates, options, extras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de estos resultados se pueden realizar observaciones sobre la **varianza** y **covarianza** de respuestas a determinadas preguntas.\n",
    "\n",
    "Primeramente, se puede observar que en la diagonal de la matriz se encuentra la **varianza** de cada atributo, destacándose las mayores varianzas para las preguntas 11, 16 y 23, mientras que las menores son para las preguntas 12, 15 y 25. Esto indica que las preguntas con menor varianza suelen ser respondidas más similarmente que las que tienen mayor varianza.\n",
    "\n",
    "Por otra parte, se analizaron ciertas preguntas en concreto en base a su **covarianza**:\n",
    "\n",
    "**Las preguntas (2, 4, 17 y 22):**\n",
    "- Debería aumentar la carga de impuestos para los ricos.\n",
    "- La ley de inclusión financiera es positiva para la sociedad.\n",
    "- Para los delitos más graves hay que bajar la edad de imputabilidad a 16 años.\n",
    "- La separación de estado y religión me parece importante.\n",
    "\n",
    "Tienen una relación negativa con una gran cantidad de preguntas, pero positiva o neutral entre sí. Se observa que:\n",
    "- Las personas que respondieron alto a una de esas preguntas tienden a responder alto a las otras y viceversa.\n",
    "- Las personas que respondieron alto a estas preguntas tienden a responder bajo a una cantidad considerable de otras preguntas y viceversa.\n",
    "\n",
    "**Las preguntas (19, 20 y 23):**\n",
    "- El feminismo moderno no busca la igualdad sino el poder.\n",
    "- La ley trans fue un error.\n",
    "- La legalización de la marihuana fue un error.\n",
    "\n",
    "Son las que tienen la mayor covarianza positiva entre sí. Se observa que:\n",
    "- Las personas que respondieron alto a una de estas tienden a responder alto en las otras y viceversa.\n",
    "\n",
    "**Las preguntas (11, 13 y 16):**\n",
    "- La pena de muerte debería ser una opción para los crímenes más serios.\n",
    "- Las FF.AA. deberían tener un rol activo en la seguridad pública.\n",
    "- Para los delitos más graves hay que bajar la edad de imputabilidad a 16 años.\n",
    "\n",
    "Tienen covarianza positiva entre sí. Se observa que:\n",
    "- Las personas que respondieron alto a una de estas tienden a responder alto en las otras y viceversa.\n",
    "\n",
    "Cerrando este análisis, se puede ver una relación conceptual entre las preguntas y sus posibles respuestas. En general, aquellas personas que responden de cierta forma en las preguntas 2, 4, 17 y 22, suelen responder de forma distinta en la mayoría de las otras preguntas, mientras que las personas que responden de cierta forma a las preguntas 19,20 y 23 y en menor medida a las preguntas 11, 13 y 16, suelen responder de forma muy similar entre ellas.\n",
    "\n",
    "Juntando esta información, se puede agrupar tendencias según su posición (acuerdo o desacuerdo) en relación con **políticas sociales** inclusivas como la ley trans, fomentar el feminismo y legalizar la marihuana. También en materias de **políticas de seguridad**, como el rol de las Fuerzas Armadas, la pena de muerte y la baja de la edad de imputabilidad agrupan tendencias con posiciones similares.\n",
    "\n",
    "Como apreciación cualitativa arbitraria, se identifica que los **sectores progresistas** posiblemente estén en desacuerdo con las preguntas planteadas, mientras que los **sectores conservadores** si están en acuerdo.\n",
    "\n",
    "Por otra parte, las preguntas con covarianza negativa, especialmente las preguntas 2 y 4 que refieren a políticas económicas sobre la inclusión financiera y la carga impositiva en clases altas, mantienen la apreciación anteriormente realizada, ya que se identifica que los **sectores progresistas** posiblemente estén de acuerdo con las preguntas planteadas, mientras que los **sectores conservadores** estén en desacuerdo. \n",
    "\n",
    "#### 4.4.5. Interpretaciones generales\n",
    "***\n",
    "\n",
    "Considerando los resultados de PCA obtenidos y englobando las interpretaciones de cada sección:\n",
    "- En el corpus original **existen** patrones o relaciones entre votantes con opiniones similares en ciertos temas\n",
    "- En el corpus reducido por **PCA**, no parece haber algún tipo de asociación entre las dos dimensiones a las que se redujo el conjunto y los resultados posibles de cada ejemplo del mismo.\n",
    "- Esta carencia de asociación en la distribución obtenida junto al hecho de haber encontrado patrones en el corpus original, es un posible indicador de que la información perdida por reducir la dimensionalidad es increíblemente alta para este conjunto.\n",
    "- Esto puede deberse a que la realidad representada por el conjunto es demasiado compleja como para reducir su dimensionalidad, al menos a dos dimensiones. Esto es respaldado por la **curva del ratio de varianza**, debido a la información no despreciable que aportan el resto de los vectores propios, la perdida de información a reducir a dos dimensiones es demasiado alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Análisis - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "\n",
    "#### 4.5.1. Proceso\n",
    "***\n",
    "Como se mencionó en anteriores secciones, el objetivo de generar clusters es encontrar posibles patrones en dichos agrupamientos.\n",
    "\n",
    "Con este conocimiento, se encaró la experimentación de la siguiente manera:\n",
    "1. Se ejecutó **K-Means** sobre el corpus, generando $k$ clusters para $k=2,3,5,10$, utilizando $10$ ejecuciones para cada $k$ y almacenando el clustering con mejor **coeficiente de Silhouette**.\n",
    "2. Se evaluó la distribución de candidatos graficándola para cada cluster con $k=2,3,5,10$.\n",
    "3. Se evaluó la distribución de partidos graficándola para cada cluster con $k=11$, calculando a su vez el ARI entre las etiquetas del corpus y de los partidos en cuestión.\n",
    "4. Se evaluó la distribución de grupos del espectro bidimensional graficándola para cada cluster con $k=5$, calculando a su vez el ARI entre las etiquetas del corpus y de los grupos en cuestión.\n",
    "5. Se evaluó la distribución de grupos del espectro unidimensional graficándola para cada cluster con $k=2$, calculando a su vez el ARI entre las etiquetas del corpus y de los grupos en cuestión.\n",
    "\n",
    "A continuación, se desglosan las cinco etapas:\n",
    "\n",
    "#### 4.5.2. Resultados\n",
    "***\n",
    "A continuación se adjunta un script que ejecuta **K-Means** sobre el corpus, graficando la distribución de ejemplos por cluster y almacenado globalmente los resultados. Es **necesario** ejecutarlo antes de ejecutar cualquier script de las siguientes subsecciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Script K-Means - Ejecutar antes que cualquier gráfica de K-MEANS\n",
    "# IMPORTANTE: Los parámetros k y executions son modificables por el usuario\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "from utils.const import *\n",
    "from model import k_means\n",
    "import processing.reader as reader\n",
    "import plotting.kMeansPlotting as kMeansPlotting\n",
    "\n",
    "# Cantidad de clusters en los que partir (EJ: 2,3,5,10)\n",
    "k = 2\n",
    "\n",
    "# Cantidad de ejecuciones de K-Means (de ser mayor a 1, calcula para cada una el coeficiente de Silhouette\n",
    "# y guarda aquel conjunto de clusters con el coeficiente más alto)\n",
    "## Advertencia: Silhouette demora segundos en ejecutar.\n",
    "executions = 2\n",
    "\n",
    "# Leer dataset de respuestas a encuesta\n",
    "candidates, dataset = reader.readDataset('../' + DATA_ENCUESTAS)\n",
    "\n",
    "# Almacenar opciones por defecto para graficar luego\n",
    "defaultOptionsKM = {\n",
    "    'kmeans_iters': executions,\n",
    "    'kmeans_analysis': KmeansAnalysis.GENERAL,\n",
    "    'kmeans_evaluations': KmeansEvaluations.NONE,\n",
    "    'candidate_division': CandidateDivision.PARTIES,\n",
    "    'from_notebook': True\n",
    "}\n",
    "\n",
    "print('K-Means - Inicio de ejecución')\n",
    "\n",
    "# Aplicar K-Means\n",
    "centroids, classes = k_means.k_means(dataset, k, defaultOptionsKM, candidates)\n",
    "\n",
    "print('K-Means - Fin de ejecución')\n",
    "print()\n",
    "\n",
    "%store dataset\n",
    "%store candidates\n",
    "%store centroids\n",
    "%store classes\n",
    "%store defaultOptionsKM\n",
    "\n",
    "kMeansPlotting.plotKMeans(dataset, candidates, centroids, classes, defaultOptionsKM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando el script anterior para valores de $k : k = 2,3,5,10$ y corriendo para cada $k$ **10 ejecuciones distintas** con el objetivo de tomar el mejor clustering, se obtuvieron los siguientes **coeficientes de Silhouette**:\n",
    "- Para $k=2$, $\\textbf{0.23}$ aproximadamente.\n",
    "- Para $k=3$, $\\textbf{0.13}$ aproximadamente.\n",
    "- Para $k=5$, $\\textbf{0.10}$ aproximadamente.\n",
    "- Para $k=10$, $\\textbf{0.08}$ aproximadamente.\n",
    "\n",
    "Teniendo dichos coeficientes en cuenta, se interpreta que para el corpus dado, el mejor clustering posible es aquel que genera **2 clusters** para todo el conjunto de datos. A priori se desconoce si existe algun patrón en relación a tal separación y a todas las demás, por lo que se procedió a realizar un análisis cualitativo de los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3. Análisis cualitativo\n",
    "***\n",
    "En la siguiente sección se adjuntan diversos scripts para la muestra de gráficas utilizando distintos clusterings generados por el script anterior. Más concretamente, se grafican para cada cluster las divisiones establecidas en la sección 4.2 (votantes según candidato votado, votantes según partido votado, votantes según posición en espectro progresismo, liberalismo, conservadurismo, centro y votantes según posición en espectro izquierda, derecha, centro).\n",
    "\n",
    "Es importante recordar dos cosas:\n",
    "- La separación de candidatos para el espectro político unidimensional y bidimensional se encuentra determinada en los archivos data/candidatosEspectro.json y data/candidatosNolan.json respectivamente.\n",
    "- Dicha separación es arbitraria y si bien se presume no del todo correcta (especialmente para el espectro bidimensional), se mantuvo con el fin de expandir las evaluaciones posibles.\n",
    "\n",
    "Teniendo esto en cuenta, el primer script grafica la división de **candidatos** por cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica KMeans - División por candidato\n",
    "options = defaultOptionsKM.copy()\n",
    "options['kmeans_analysis'] = KmeansAnalysis.CANDIDATES\n",
    "kMeansPlotting.plotKMeans(dataset, candidates, centroids, classes, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada la enorme cantidad de clasificaciones (24 candidatos), es difícil apreciar conclusiones e interpretaciones de esta grafica para cualquier valor de $k$. Es por esto, que se refina el análisis con generalizaciones para cada clase. Con este objetivo, se adjunta a continuación un script que muestra la división en clusters de cada **partido político**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica KMeans - División por partido político\n",
    "options = defaultOptionsKM.copy()\n",
    "options['kmeans_analysis'] = KmeansAnalysis.PARTIES\n",
    "options['candidate_division'] = CandidateDivision.PARTIES\n",
    "kMeansPlotting.plotKMeans(dataset, candidates, centroids, classes, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para el caso de $k = 2$**\n",
    "\n",
    "Se obtuvo el resultado cuyos clusters más marcan una diferencia respecto a las clasificaciones de cada ejemplo. Si bien nunca se pudo alcanzar una separación perfecta, siempre se llegó a una separación razonable entre partidos opuestos, encontrándose el *Frente Amplio* y la *Unidad Popular* contra el *Partido Nacional*, el *Partido Colorado* y el *Partido de la Gente* (lo cual tiene sentido dado pertenecen a espectros similares entre sí).\n",
    "\n",
    "**Para el caso de $k = 3,5,10$**\n",
    "\n",
    "Al escalar la cantidad de conjuntos a los cuales agrupar, se observa que en lugar de refinar grupos cuya densidad era poco uniforme (por ejemplo el cluster que contiene al *Partido Independiente* y al *Partido Colorado* para $k = 2$), el resultado obtenido son sub-divisiones con similar densidad de clusters generados previamente.\n",
    "\n",
    "Con el objetivo de refinar aún más el análisis y encontrar patrones en los clusters obtenidos, se adjunta a continuación un script que muestra la división en clusters de cada **grupo del espectro bidimensional**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica KMeans - División por espectro bidimensional (conservadurismo, progresisimo, liberalismo, centro)\n",
    "options = defaultOptionsKM.copy()\n",
    "options['kmeans_analysis'] = KmeansAnalysis.PARTIES\n",
    "options['candidate_division'] = CandidateDivision.NOLAN\n",
    "kMeansPlotting.plotKMeans(dataset, candidates, centroids, classes, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para el caso de $k = 2$**\n",
    "\n",
    "Se observa que efectivamente la agrupación para dos clusters logra separar bastante bien los espectros ideológicos de **conservadurismo** y **liberalismo** de **progresismo**. El hecho de que los candidatos conservadores y liberales se agrupen está fuertemente relacionado a la clasificación sesgada mencionada anteriormente, ya que ambos representan partidos de derecha.\n",
    "\n",
    "**Para el caso $k = 3$**\n",
    "\n",
    "Una observación muy interesante es que si bien dos de los clusters generados agrupan una gran concentración de solamente ejemplos clasificados como **progresismo* y **consdervadurismo**, el cluster restante agrupa elementos indiferentemente de su clasificación.\n",
    "\n",
    "**Para el caso $k = 5, 10$**\n",
    "\n",
    "Se observa que al aumentar $k$ los conjuntos con mayor proporción de **conservadurismo** son cada vez más \"puros\". Este mismo hecho también se cumple para el grupo **progresismo**, por lo que podría tener sentido agrupar ciertos clusters tras finalizado el algoritmo. No obstante, un resultado bastante similar ya se obtenia con $k = 2$. Esto respalda la observación de que los conjuntos previos se están subdividiendo entre sí sin formar mejores refinaciones del espacio de clasificación.\n",
    "\n",
    "Tomando en cuenta los patrones encontrados en esta división, se adjunta a continuación un script que muestra la división en clusters de cada grupo del espectro **unidimensional:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gráfica KMeans - División por espectro unidimensional\n",
    "options = defaultOptionsKM.copy()\n",
    "options['kmeans_analysis'] = KmeansAnalysis.PARTIES\n",
    "options['candidate_division'] = CandidateDivision.SPECTRUM\n",
    "kMeansPlotting.plotKMeans(dataset, candidates, centroids, classes, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independiente al caso, se puede observar que los resultados son sumamente similares a la gráfica anterior. De hecho, si se establece el paralelismo **progresismo - izquierda** y **conservadurismo, liberalismo - derecha**, se puede ver que los resultados son idénticos, confirmando que la mejor separación es según la idea de izquierda y derecha utilizando $k=2$ clusters.\n",
    "\n",
    "Si se observan los resultados anteriores, también es posible confirmar que de aquellos partidos catalogados como **centro**, el **Partido Digital** se encuentra en su gran mayoría en el cluster \"de izquierda\" (aquel que tiene más abundancia de votantes de candidatos de izquierda), mientras que el **Partido Verde Animalista** se encuentra en su gran mayoría en el cluster \"de derecha\". En lo que concierne al resto de partidos catalogados como **centro**, tanto **La Alternativa** como el **Partido Ecologista Radical Intransigente** y el resto, se encuentran distribuidos en ambos clusters de forma similar.\n",
    "\n",
    "Habiendo determinado de forma cualitativa que el mejor clustering es aquel que separa en dos clusters, se procede a calcular el **ARI** de cada posible división para buscar confirmar esta intuición."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.4. Evaluación de Ajusted Rand Index (ARI)\n",
    "***\n",
    "A continuación se evalúan los casos de $k=2,3,5,11$ asociando cada una a las divisiones por espectros (unidimensional y bidimensional) y la división por partidos. Debido a que para el cálculo de **ARI** es esencial trabajar con asociaciones especificas de estos casos, se presenta un script adicional de ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executions = 5\n",
    "k = 5 # 2, 3, 5, 11\n",
    "\n",
    "options = defaultOptionsKM.copy()\n",
    "options['kmeans_iters'] = executions\n",
    "options['kmeans_evaluations'] = KmeansEvaluations.ARI\n",
    "options['kmeans_analysis'] = KmeansAnalysis.PARTIES\n",
    "\n",
    "if k == 2:\n",
    "    options['candidate_division'] = CandidateDivision.DUAL_SPECTRUM\n",
    "elif k == 3:\n",
    "    options['candidate_division'] = CandidateDivision.SPECTRUM\n",
    "elif k == 5:\n",
    "    options['candidate_division'] = CandidateDivision.NOLAN\n",
    "elif k == 11:\n",
    "    options['candidate_division'] = CandidateDivision.PARTIES\n",
    "\n",
    "if k == 2 or k ==3 or k == 5 or k == 11:\n",
    "    # Aplicar K-Means\n",
    "    centroids, classes = k_means.k_means(dataset, k, options, candidates)\n",
    "\n",
    "    kMeansPlotting.plotKMeans(dataset, candidates, centroids, classes, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando el script anterior para valores de $k : k = 2,3,5,11$ y corriendo para cada $k$ **10 ejecuciones distintas** con el objetivo de tomar el mejor clustering, se obtuvo que los siguientes **ARI**.\n",
    "- Para $k=2$, $\\textbf{0.62}$ aproximadamente.\n",
    "- Para $k=3$, $\\textbf{0.45}$ aproximadamente.\n",
    "- Para $k=5$, $\\textbf{0.23}$ aproximadamente.\n",
    "- Para $k=10$, $\\textbf{0.13}$ aproximadamente.\n",
    "\n",
    "Teniendo dichos datos en cuenta y toda la información anteriormente observada, se confirma que para el corpus dado, el mejor clustering posible es aquel que genera **2 clusters** para todo el conjunto de datos, separando en cierta medida, a aquellos candidatos con ideología de izquierda o centroizquierda de aquellos con ideología de derecha o centroderecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.5. Interpretaciones generales\n",
    "***\n",
    "\n",
    "Respecto a las agrupaciones en partidos:\n",
    "- Se presume que la razón por la cual los conjuntos que agrupaban ejemplos que clasificaban con candidatos del *Partido Nacional* también agrupaban ejemplos cuya clasificación eran candidatos del *Partido Colorado* es debido a que las respuestas a preguntas que clasificaron con algunos candidatos del *Partido Nacional* son muy similares a las que clasificaron con algunos candidatos del *Partido Colorado*. Esta conjetura es razonablemente respaldada dado que nunca se pudo alcanzar una buena separación de estos entre sí (ni siquiera para $k = 3$, que era el valor candidato para alcanzar este resultado dada la proporción de votantes para cada partido).\n",
    "\n",
    "- Se presume que la razón de que los partidos menos populares quedaran desperdigados por todos los clusters puede en gran parte deberse a que por la baja ocurrencia de estos ejemplos (o a causa del ruido en los datos), la inteligencia artificial de *aquienvoto.uy* no haya podido identificar del todo las respuestas que consistentemente se identifiquen con candidatos de dichos partidos.\n",
    "\n",
    "- Incrementar $k$ fue algo que perdió valor mucho más rápido de lo que se esperaba, para ninguno de los casos en la experimentación se pudo alcanzar una mejor clasificación que con $k=2$ debido al fenómeno de que los nuevos clusters generados acababan siendo subdivisiones de similar densidad a clusters anteriores (en lugar de refinaciones de los mismos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusiones\n",
    "***\n",
    "#### 5.1. Respecto a los Datos\n",
    "***\n",
    "- La política es una ciencia social y por lo tanto, está lejos de ser exacta. Existen múltiples variables a tomar en cuenta al momento de hacer interpretaciones sobre cualquier conjunto de datos, siendo altamente probable la presunción de hipótesis incorrectas.\n",
    "- Definir un conjunto de preguntas tendenciosas puede sesgar la generación de grupos y la asignación de candidatos a votantes.\n",
    "- Definir respuestas numéricas ante preguntas de materias muy generales no sólo simplifica excesivamente un fenómeno tan complejo como la ideología política, sino que también asume que el sujeto en cuestión cuenta con los conocimientos necesarios para responder acorde a su ideología.\n",
    "\n",
    "#### 5.2. Respecto a los Resultados\n",
    "***\n",
    "- La reducción de dimensionalidad puede resultar útil en ciertos conjuntos de datos, pero en este caso se perdió demasiada información y por lo tanto fue imposible determinar algún patrón.\n",
    "- Por otra parte, el clustering demostró resultar de utilidad y separar en clusters con proporciones distintas de cada grupo político. No obstante, la separación estuvo lejos de ser 100% acertada. Este hecho puede ser un indicador de que el modelo no es lo suficientemente bueno para el conjunto de datos, o que el espectro de separación es más complejo de lo que se determinó.\n",
    "- El separar en grupos conceptuales y asignar a cada candidato un grupo arbitrariamente, particularmente los del espectro político, puede haber influido negativamente en el análisis, ya que puede haber sesgado incorrectamente cierto grupo.\n",
    "- De todas formas, se logró determinar que el mejor clustering obtenido es aquel que generó 2 clusters para todo el corpus, comprobandose también que la relación entre la ideología de los candidatos y el cluster al que pertenecen es estrecha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Referencias\n",
    "***\n",
    "[1] **Teoría analítica de la política** - *Melvin J. Hinich y Michael C. Munger* (2003)\n",
    "\n",
    "[2] **Un centro vacío de candidatos: evaluando modelos espaciales para las elecciones presidenciales en Uruguay** - *Juan Andrés Moraes y Diego Luján (2016)*\n",
    "\n",
    "[3] **The Political Spectrum: A Bi-Dimensional Approach** - *Maurice Bryson y William McDill* (1968)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
