{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 4 - Análisis de datos (PCA, K-Means)\n",
    "\n",
    "### Grupo 07:\n",
    "     - Renzo Gambone C.I. 5.155.486-4\n",
    "     - Germán Ouviña C.I. 4.823.566-1\n",
    "     - Leandro Rodríguez C.I 4.691.736-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "***\n",
    "### 1.1. Objetivo\n",
    "***\n",
    "El objetivo de esta tarea fue analizar bajo distintos enfoques el corpus _aquienvoto.uy_. Más específicamente, se tuvieron en cuenta los siguientes lineamientos:\n",
    "- Implementar un algoritmo de reducción de dimensionalidad (algoritmo PCA) para visualizar los datos.\n",
    "- Implementar un modelo de aprendizaje no supervisado (algoritmo K-means) para agrupar los datos.\n",
    "- Analizar los resultados intermedios generados en cada enfoque, buscando patrones y extrayendo información de utilidad en el proceso.\n",
    "- Analizar los resultados finales generados en cada enfoque, contestando a las preguntas planteadas en el práctico y a otras surgidas de las observaciones del corups.\n",
    "- Enfocar dichos análisis desde un punto de vista cualitativo, teniendo en cuenta métricas cuantitativas con el fin de orientar dicho análisis.\n",
    "\n",
    "### 1.2. Entrega\n",
    "***\n",
    "La entrega de esta tarea consta de dos grandes componentes:\n",
    "- **Informe** en formato de Jupyter Notebook (este informe).\n",
    "- **Programa** que permite analizar el conjunto de datos utilizando **PCA** o **KMeans**, generar distintas gráficas, evaluar el desempeño de los algoritmos utilizados, entre otras utilidades.\n",
    "\n",
    "El objetivo del **informe** es centralizar la información relativa a los distintos métodos de análisis empleados, así como los resultados obtenidos y las conclusiones generadas. En esta ocasión, es posible ejecutar el código principal completamente desde el informe, agregandose las funciones necesarias en las secciones correspondientes (ver sección 1.3. para una descripción detallada sobre el código encontrado en cada sección).\n",
    "\n",
    "Por otra parte, el **programa** ofrece una interfaz en consola que permite la ejecución de ambos algoritmos acorde a distintas configuraciones paramétricas, así como generar todos los datos mostrados en el informe. Si bien todas estas herramientas fueron pensadas para uso del grupo, en el archivo *README.md* se adjunta una sencilla guía de como utilizarlas.\n",
    "\n",
    "A diferencia de las anteriores entregas, el **programa** y su interfaz se agregó como una herramienta puramente auxiliar. Como se mencionó anteriormente, es posible ejecutar el código principal en el **informe**. De todas formas, con el fin de facilitar la lectura, se redujo el uso de código dentro del informe lo máximo posible. Para un mayor entendimiento de los algoritmos empleados, referir a los archivos de código fuente.\n",
    "\n",
    "### 1.3. Estructura\n",
    "***\n",
    "Dado que en esta ocasión el objetivo principal está en el **análisis** y no en la **construcción** de uno o varios modelos, tanto la estructura como las tareas realizadas difieren bastante con respecto a las anteriores entregas. A continuación se adjunta una breve introducción del contenido de cada sección:\n",
    "\n",
    "- En la **Sección 2 (Enfoque)** se define y contrasta los dos grandes enfoques utilizados a lo largo de la tarea (reducción de dimensionalidad y aprendizaje no supervisado), manteniéndose dicha dualidad en el resto de secciones.\n",
    "- En la **Sección 3 (Diseño)** se tratan los aspectos matemáticos y técnicos de cada enfoque, definiendo los algoritmos empleados, formas de evaluación generales, etc.\n",
    "- En la **Sección 4 (Experimentación)** se define la metodología empleada a la hora de experimentar, así como el proceso de generación de resultados, los resultados en sí y el análisis posterior de los mismos. También se desarrolla una sección con información generada durante la investigación previa realizada en relación al marco teórico del corpus, justificando la toma de decisiones de los distintos experimentos realizados. \n",
    "- En la **Sección 5 (Conclusiones)** se adjuntan las conclusiones generales obtenidas en el desarrollo de la tarea, organizandose las mismas según distintas subsecciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enfoque\n",
    "***\n",
    "Como se mencionó en la sección anterior, el objetivo principal de la tarea es estudiar el corpus de _aquienvoto.uy_. El interés general radica en analizar la relación entre los potenciales votantes, buscando patrones en sus respuestas y enfocandose en el análisis cualitativo, es decir, priorizar las observaciones e interpretaciones de los datos.\n",
    "\n",
    "Existen mútliples estrategias posibles para encarar dicho problema, por lo que a continuación se definen los dos enfoques utilizados:\n",
    "\n",
    "### 2.1. Enfoque 1 - Reducción de Dimensionalidad\n",
    "***\n",
    "Por un lado, es posible analizar los datos sin utilizar ningún modelo de aprendizaje automático. Dado que el corpus cuenta con **26 atributos**, el primer enfoque se centra en la **reducción de dimensionalidad** del corpus en sí, buscando reducir la cantidad de dimensiones para poder visualizar los datos y buscar una relación entre votantes graficamente.\n",
    "\n",
    "Dicho enfoque puede presentar varios problemas, ya que para poder graficar los datos es necesario pasar de **26 dimensiones** a **2 dimensiones**, siendo factible una pérdida importante de información en el proceso. De hecho, dado un corpus específico, dependiendo de la técnica de reducción de dimensionalidad empleada y de la relación de los datos en sí, es posible determinar que tanto impactará la reducción de dimensiones en la integridad del corpus.\n",
    "\n",
    "De todas formas, si el corpus cuenta con una estructura \"resistente\" a la reducción de dimensiones y el algoritmo utilizado es compatible con dicha estructura, sería posible observar o descartar patrones entre los votantes.\n",
    "\n",
    "Tomando en cuenta lo anterior, existen múltiples técnicas de reducción de dimensionalidad. En el contexto de esta tarea se utilizará **PCA (Principal Component Analysis)**. En la sección 3.1 se especifican los aspectos técnicos de dicho algoritmo y en la sección 4.4 los resultados de la experimentación y análisis empleando dicha técnica.\n",
    "\n",
    "### 2.2. Enfoque 2 - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "Por otra parte, es posible analizar los datos utilizando modelos de aprendizaje automático, más concretamente modelos que implementen técnicas de **clustering** o **agrupamiento**, siendo está una forma de **aprendizaje no supervisado**.\n",
    "\n",
    "Dicho enfoque genera distintos grupos de votantes, basandose en su distribución en un espacio de **26 dimensiones**. Dependiendo de la naturaleza del corpus, esto puede generar problemas. Al igual que en el anterior enfoque, es posible determinar la cantidad óptima de grupos a generar (ya que a priori, esta es desconocida).\n",
    "\n",
    "Tomando en cuenta lo anterior, existen múltiples técnicas de clustering. En el contexto de esta tarea se utilizará **K-Means**. En la sección 3.2 se especifican los aspectos técnicos de dicho algoritmo y en la sección 4.5 los resultados de la experimentación y análisis empleando dicha técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diseño\n",
    "***\n",
    "En esta sección se detallan las características del diseño utilizado para construir las herramientas de análisis para cada enfoque, profundizandose las estrategias y algoritmos empleados, así como otros puntos, como el procesamiento previo al análisis y/o entrenamiento y la evaluación posterior al mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Enfoque 1 - Reducción de Dimensionalidad\n",
    "***\n",
    "\n",
    "#### 3.1.1. Algoritmo\n",
    "***\n",
    "Como se mencionó en la sección anterior, el algoritmo empleado para reducir las dimensiones del corpus fue **PCA**. Como su nombre indica, el algoritmo hace uso de los **componentes principales** de una matriz. Se define como componente principal de una matriz al mayor **valor propio** de la misma.\n",
    "\n",
    "En términos generales, el algoritmo consiste en calcular $k$ componentes principales de cierta matriz, formar otra matriz con vectores propios correspondientes a cada componente principal y transformar el subespacio del corpus multiplicando cada vector del mismo por la matriz de vectores propios. Es importante destacar que $k$ es la cantidad de dimensiones que se quiere mantener, teniendo que ser entero positivo menor a la cantidad de dimensiones actual.\n",
    "\n",
    "Dado que existen múltiples métodos para implementar **PCA**, la matriz que se obtiene y por ende los componentes principales de la misma dependen del método. No obstante, los resultados son ortogonales al método, es decir, la reducción de dimensionalidad obtenida es independiente al método utilizado.\n",
    "\n",
    "Teniendo esto en cuenta, con el objetivo de validar métodos se implementaron dos formas distintas de **PCA**:\n",
    "\n",
    "##### 3.1.1.1. Matriz de Covarianza\n",
    "***\n",
    "\n",
    "Sea un vector de atributos $A = [A_1, ..., A_n]$ y sea $\\mu_i$ la media del atributo $A_i$, se define la **covarianza** entre dos atributos $A_i, A_j$ como la varianza conjunta de ambos atributos, utilizando la siguiente expresión:\n",
    "\n",
    "$$ Cov(A_i, A_j) = E((A_i - \\mu_i)(A_j - \\mu_j)) $$\n",
    "\n",
    "En cierta forma, la **covarianza** entre $A_i$ y $A_j$ representa la relación entre la varianza de ambos atributos por separado. Una **covarianza positiva** indica que ambos atributos crecen de forma _directamente proporcional_, mientras que una **covarianza negativa** indica que ambos atributos crecen de forma _inversamente proporcional_.\n",
    "\n",
    "De esta forma, la **matriz de covarianza** $M$ de un conjunto de datos $D$ con un vector de atributos $A$ se define como la covarianza de cada posible par de atributos de $A$, utilizando la siguiente expresión:\n",
    "\n",
    "$$M(D,A) = (Cov(A_i,A_j))_{ij} : 1 \\leq i, j \\leq |A| $$\n",
    "\n",
    "Como se mencionó anteriormente, **PCA** utiliza los componentes principales de una matriz para reducir la dimensión del subespacio que el corpus representa. En este método, la matriz empleada es nada más y nada menos que la **matriz de covarianza** para el corpus $D$ y sus atributos $A$.\n",
    "\n",
    "Teniendo esto en cuenta, el algoritmo implementado consiste en los siguientes pasos:\n",
    "1. Sustraer la media $\\mu_i$ para cada atributo $A_i$ del conjunto de datos (esto transforma al corpus en un conjunto de datos con media 0 para cada atributo).\n",
    "2. Calcular la **matriz de covarianza** $M(D,A)$.\n",
    "3. Calcular los **valores propios** de $M(D,A)$ y obtener un **vector propio** para cada valor.\n",
    "4. Elegir los $k$ **componentes principales** de $M(D,A)$ y formar la matriz $W$ con los $k$ vectores propios correspondientes a los valores propios elegidos (y en el orden adecuado).\n",
    "5. Sea $D$ la matriz que representa al corpus original, se calcula $T = DW$, teniendo esta $k$ dimensiones.\n",
    "\n",
    "\n",
    "##### 3.1.1.2. Descomposición SVD\n",
    "***\n",
    "\n",
    "Otro potencial método para obtener una matriz de la cual extraer los componentes principales es la **descomposición SVD** o **descomposición en valores singulares**. \n",
    "\n",
    "Sea $M \\in R_{m \\times n}$ una matriz, su descomposición SVD se define como una factorización del tipo $M = USV$ donde $U,S,V$ son matrices que cumplen las siguientes propiedades:\n",
    "- La matriz $U \\in R_{m \\times m}$ está formada por vectores propios de $M . M^t$, siendo $U$ ortonormal.\n",
    "- La matriz $V \\in R_{n \\times n}$ está formada por vectores propios de $M^t . M$, siendo $V$ ortonormal.\n",
    "- La matriz $S \\in R_{m \\times n}$ está formada por los **valores singulares** de $M$ en su diagonal principal ordenados de mayor a menor.\n",
    "\n",
    "Cabe destacar que los **valores singulares** de una matriz son las raíces cuadradas de sus **valores propios** por lo que el mayor valor singular de una matriz es la raíz cuadrada del componente principal de dicha matriz.\n",
    "\n",
    "El método implementado se sirvió de obtener los valores propios de la matriz de covarianza, utilizando luego aritmética de matrices para obtener una matriz $T$ que represente al corpus $D$ con dimensionalidad reducida.\n",
    "\n",
    "Dado que el método con **SVD** no aporta más que una validación a la hora de comparar con el método anterior, no se entrará en detalles de su implementación. Para ver dicha implementación, referir al código fuente del módulo _pca.py_.\n",
    "\n",
    "#### 3.1.2. Evaluación\n",
    "***\n",
    "\n",
    "Los algoritmos de reducción de dimensionalidad tienen distinto rendimiento dependiendo de su forma de operar y del conjunto de datos sobre el que trabajen. En cualquier caso, cuanto más se reducen las dimensiones de un corpus, más información se pierde.\n",
    "\n",
    "Normalmente, al momento de reducir las dimensiones de un conjunto de datos utilizando **PCA**, surge la necesidad de decidir cuántas componentes se van a considerar para que no se pierda el significado que contienen los datos en sí. Para esto se suele calcular una métrica denominada **ratio de varianza explicada**, la cual permite determinar en términos generales la pérdida de información provocada al reducir las dimensiones para cada posible $k$ que se tome. De esta forma, dependiendo de donde se ponga el límite, el menor $k$ que supere ese límite sería la cantidad \"ideal\" de componentes a tomar.\n",
    "\n",
    "Para calcular el **ratio de varianza explicada** se procesan los valores propios, calculándose la suma acumulativa para cada cantidad de componentes consideradas en cada caso. De esta forma, para cierto $k$, se obtiene que esta métrica representa la suma de los $k$ componentes principales sobre la suma de todos los valores propios. Cuanto mayor es este valor, mejor es esa elección de $k$. Lógicamente, cuanto mayor es $k$, mayor es el ratio de varianza explicada (de hecho, es creciente).\n",
    "\n",
    "En este contexto en particular, se sabe que el interés de utilizar **PCA** es reducir a **2 dimensiones**, fijando $k = 2$. No obstante, utilizando el **ratio de varianza explicada**, se puede conocer que tanta información se está perdiendo. En la sección de experimentación 4.4 se muestran los resultados de la evaluación de **PCA** y sus implicancias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Enfoque 2 - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "\n",
    "#### 3.2.1. Modelo\n",
    "***\n",
    "Las técnicas de **aprendizaje no supervisado** difieren sustancialmente de las técnicas implementadas en anteriores entregas, las cuales son del grupo de técnicas de **aprendizaje supervisado**. En esencia, el aprendizaje supervisado utiliza para entrenar un conjunto de datos con clasificaciones conocidas, mientras que el aprendizaje no supervisado, en lugar de vincular elementos del conjunto con su clasificación, se concentra en observar patrones e irregularidades en la distribución de los elementos en el espacio comprendido por sus atributos.\n",
    "\n",
    "Dentro de las técnicas de aprendizaje no supervisado, existe una que se denomina **clustering**, la cual tiene como objetivo agrupar los elementos del conjunto de datos en subconjuntos disjuntos denominados **clusters**, los cuales agrupan elementos similares de alguna forma en cuanto a sus atributos, siguiendo distintas nociones dependiendo del modelo implementado.\n",
    "\n",
    "Cabe mencionar que los resultados obtenidos no necesariamente reflejan conclusiones que sean de valor para un clasificador, sino que los mismos están orientados netamente hacia la distribución de los elementos en el conjunto de atributos. Observar como los elementos están distribuidos no es trivial para un corpus con alta dimensionalidad, pero permite destacar anomalías en el conjunto y agrupar ejemplos \"similares\". Por otra parte, es posible que la agrupación en clusters esté vinculada con la clasificación de los elementos de un cluster, sin embargo esta presunción no puede realizarse sin la experimentación adecuada.\n",
    "\n",
    "Teniendo en cuenta las generalidades anteriormente descritas, en el contexto de esta tarea se implementó el modelo de **clustering** denominado **K-Means Clustering** (a partir de ahora denominado **K-Means**), el cual representa el conjunto de datos como un espacio de **$n$ dimensiones** (siendo $n$ la cantidad de atributos) e itera sobre el mismo, buscando determinar los anteriormente mencionados **clusters**.\n",
    "\n",
    "Se definen más formalmente las siguientes nociones:\n",
    "* $D =$ Conjunto de entrenamiento.\n",
    "* $C_D =$ Conjunto de posibles clasificaciones para $d \\in D$.\n",
    "* $KMeans_{(D,k)} =$ Agrupación del conjunto $D$ en $k$ clusters, los cuales son representados por sus **centroides**, los cuales son coordenadas en el espacio de los atributos que determinan la posición espacial de cada cluster.\n",
    "* Para un ejemplo $d \\in D$, es posible determinar a que cluster pertenece asociandolo al cluster cuyo centroide esté más próximo a el mismo.\n",
    "\n",
    "Los detalles sobre el algoritmo de entrenamiento y de agrupación de nuevos ejemplos se expanden en la siguiente sección.\n",
    "\n",
    "#### 3.2.2. Algoritmo\n",
    "***\n",
    "\n",
    "##### 3.2.2.1. Algoritmo de entrenamiento\n",
    "***\n",
    "El algoritmo **K-Means** implementado sigue los lineamientos del visto en el teórico, por lo que no es necesario aclarar ningúna particularidad de su implementación, más que el hecho de que el umbral de variación entre los centroides obtenidos en una iteración y la siguiente debe ser de 0.0001 o menor para que el algoritmo considere haber terminado.\n",
    "\n",
    "##### 3.2.2.2. Algoritmo de agrupación\n",
    "***\n",
    "Con el objetivo de separar conceptualmente de la clasificación vista en modelos de aprendizaje supervisado, se le denomina **agrupación** a determinar el cluster al que debería pertenecer un nuevo ejemplo que no se encontraba en el conjunto de datos original.\n",
    "\n",
    "Dada la naturaleza de **K-Means**, el algoritmo de agrupación es posible y ligero en terminos de computo. Consiste en simplemente calcular la distancia euclídea del ejemplo a agrupar para cada uno de los centroides de cada cluster. Aquel centroide que se encuentre más cerca del ejemplo, representará al cluster al que dicho ejemplo pertenece.\n",
    "\n",
    "Es crucial mencionar que nuevos ejemplos podría afectar a los clusters generados originalmente, cambiando la distribución de ejemplos e incluso la ubicación de los centroides. Por ende, para ciertos escenarios es incorrecto utilizar este enfoque de agrupación de nuevos ejemplos y debe correrse el algoritmo de entrenamiento otra vez. Dado que la implementación para esta tarea no tiene en cuenta la agrupación de nuevos ejemplos en ningún punto, se consideró un enfoque posible. \n",
    "\n",
    "#### 3.2.4. Evaluación\n",
    "***\n",
    "Debido a que el aprendizaje no supervisado no apela a generar un clasificador y validarlo con un subconjunto de prueba, no es posible evaluar su rendimiento utilizando las métricas de tareas anteriores. De hecho, a priori no existe una clasificación \"correcta\".\n",
    "\n",
    "Teniendo esto en cuenta, se evaluaron los potenciales problemas del modelo **K-Means** y se planteó el uso de distintas métricas para determinar que clusters agrupan mejor a los datos. A continuación se especifican dichos problemas:\n",
    "\n",
    "##### 3.2.4.1. Problemas del modelo\n",
    "***\n",
    "\n",
    "**K-Means** tiene dos problemas principales:\n",
    "\n",
    "* La función de costo a minimizar no es convexa, y por ende, tiene óptimos locales. A su vez el algoritmo **K-Means** tiene solamente componentes de *explotación* y no de *exploración*, por lo que es propenso a quedarse atrapado en el primer óptimo local que encuentre.\n",
    "\n",
    "* La elección del número $k$ no es trivial para cada conjunto de datos. Un valor de $k$ bajo será más eficiente computacionalmente y tendrá una función de costo más fácil de optimizar, pero las agrupaciones en clusters no necesariamente reflejarán una clara distribución de los elementos en el conjunto. Por otra parte, un valor de $k$ alto puede tener una mayor cantidad de óptimos locales y puede subdividir el conjunto en más clusters de lo que realmente tiene sentido para la distribución del conjunto.\n",
    "\n",
    "Sabiendo esto, en la experimentación se decidió implementar ciertas estrategias con el objetivo de palear la influencia de estos problemas, siendo las mismas:\n",
    "\n",
    "* Realizar $n$ ejecuciones independientes del algoritmo (siendo $n$ un parámetro de entrada), manteniendo los resultados de mejor calidad, acorde a cierta métrica (la cual se define en la siguiente subsección). De esta forma se busca dar un componente de *exploración* al algoritmo.\n",
    "\n",
    "* Realizar ejecuciones para distintos valores de $k$, estudiando su comportamiento en relación al cambio de $k$.\n",
    "\n",
    "En la sección de experimentación 4.5 se profundiza sobre los enfoques tomados y los resultados obtenidos gracias a ello.\n",
    "\n",
    "##### 3.2.4.2. Métricas empleadas\n",
    "***\n",
    "\n",
    "Para cuantificar la calidad del agrupamiento en clusters del algoritmo **K-Means** se emplearon dos métricas distintas que ofrecen distintas nociones sobre la calidad de los clusters elegidos. Las implementaciones de dichas métricas son las brindadas por la biblioteca *sklearn* de Python.\n",
    "\n",
    "Es importante mencionar que, de cierta forma, estas métricas atentan contra la idea del aprendizaje no supervisado, ya que se utilizan clasificaciones conocidas para evaluar la calidad de los clusters. Dichas clasificaciones no siempre están disponibles y a veces ni siquiera existen. Sin embargo, en este contexto se cuenta con esos datos, por lo cual se utilizarán para encontrar patrones.\n",
    "\n",
    "Las métricas son las siguientes:\n",
    "\n",
    "###### 3.2.4.2.1. Silhouette\n",
    "***\n",
    "El **coeficiente de Silhouette** evalúa los resultados del modelo observando la densidad de cada cluster según la clasificación original de cada elemento. Un coeficiente alto se relaciona con que los resultados tienen clusters \"bien definidos\".\n",
    "\n",
    "Dados los resultados:\n",
    "* **a:** La distancia promedio entre una muestra y todos los otros puntos de la misma clase.\n",
    "* **b:** La distancia promedio entre una muestra y todos los otros puntos en el cluster más proximo.\n",
    "\n",
    "Se define el **coeficiente de Silhouette** como: $$s = \\frac{b-a}{max(a,b)}$$\n",
    "\n",
    "Para un conjunto de muestras, se toma el promedio general del **coeficiente de Silhouette** para cada muestra.\n",
    "\n",
    "###### 3.2.4.2.2. Adjusted Random Index (ARI)\n",
    "***\n",
    "La métrica **ARI** se encuentra comprendida en el intervalo $[-1..1]$, y dadas dos listas con valores \"reales\" y \"predecidos\" mide la similaridad entre los elementos de las mismas, ignorando posibles permutaciones. En esencia, esta métrica es útil para comprobar si efectivamente cada cluster está vinculado con las clasificaciones originales del conjunto o con otros posibles agrupamientos conocidos.\n",
    "\n",
    "A diferencia del *coeficiente de Silhouette*, esta métrica sólo tiene sentido para determinados valores de $k$ (valores a los cuales se pueda asociar una clasificación), dado que no evalúa según la densidad de cada cluster, sino según como el conjunto entero fue agrupado en clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimentación\n",
    "***\n",
    "En esta sección se detallan los aspectos relacionados a las distintas pruebas realizadas, tratando tanto los lineamientos previos como los análisis realizados sobre los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Metodología\n",
    "***\n",
    "\n",
    "Dado el objetivo de buscar patrones en el corpus, se utilizó una metodología de experimentación particular, dividiendola en las siguientes etapas:\n",
    "\n",
    "1. **Investigación previa:** Al tratarse de un corpus con un contexto específico (posibles votaciones en elecciones nacionales), fue necesario realizar una investigación genérica sobre dicho contexto para poder evaluarlo lo mejor posible. En resumen, se investigó sobre generalidades de la política partidaria, análisis estadísticos en poblaciones de votantes, variables indicadoras, entre otros. En la sección 4.2 se profundiza la información recabada y los sesgos introducidos por la interpretación de la misma.\n",
    "<br><br>\n",
    "2. **Análisis del corpus:** Habiendo realizado el sondeo de información básica sobre el tema, se analizó la estructura del corpus, especulando sobre su correctitud. En la sección 4.3 se estudian las posibles ventajas y desventajas de esta estructura.\n",
    "<br><br>\n",
    "3. **Generación de resultados:** Una vez realizada la investigación y el análisis preliminar, se ejecutaron distintas configuraciones de ambos enfoques con el fin de observar los datos resultantes. En la sección 4.4 se expanden las elecciones de configuraciones utilizadas, la justificación de dichas elecciones, los resultados más importantes y se adjuntan los scripts que fueron implementados.\n",
    "<br><br>\n",
    "4. **Análisis de enfoques:** Teniendo los resultados generados, se realizó un estudio de los mismos con un enfoque de análisis cualitativo, utilizando los conocimientos generados en las etapas previas. Se analizaron métricas descriptivas de ambos algoritmos, también realizandose observaciones e interpretaciones de cada resultado.En la sección 4.5 se expanden estos puntos y se adjuntan los scripts necesarios para evaluar el desempeño del algoritmo utilizado y la calidad de los resultados analizados.\n",
    "<br><br>\n",
    "5. **Contraste entre enfoques:** Para finalizar, se contrastó de forma general los resultados, observaciones e interpretaciones de cada enfoque, con el objetivo de detectar alguna contradicción o parámetro en común que permita rescatar o descartar conclusiones. En la sección 4.6 se tratan estos temas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Marco Teórico\n",
    "***\n",
    "\n",
    "Links:\n",
    "- https://es.wikipedia.org/wiki/Gr%C3%A1fico_de_Nolan\n",
    "- https://es.wikipedia.org/wiki/Espectro_pol%C3%ADtico\n",
    "- https://es.wikipedia.org/wiki/Izquierda_y_derecha_(pol%C3%ADtica)\n",
    "- https://es.wikipedia.org/wiki/Partido_pol%C3%ADtico\n",
    "\n",
    "Separaciones:\n",
    "- Candidatos\n",
    "- Partidos\n",
    "- Izquierda / derecha\n",
    "- Grafica de Nolan 1\n",
    "- Grafica de Nolan 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Análisis - Corpus\n",
    "***\n",
    "\n",
    "Acá analizamos el corpus usando los datos de la parte anterior.\n",
    "- Hay 3 grupos de preguntas que tal vez no son representativos\n",
    "- Hay 26 preguntas que tal vez no son representativas\n",
    "- Los puntajes tal vez no son representativos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Análisis - Reducción de dimensionalidad\n",
    "***\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1. Proceso\n",
    "***\n",
    "Acá mencionar que cosas corrimos\n",
    "\n",
    "#### 4.4.2. Resultados\n",
    "***\n",
    "Acá mostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que corre PCA y muestra resultados generales (grafica sin diferenciacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3. Análisis cualitativo\n",
    "***\n",
    "Acá no se compara, se hacen observaciones e interpretaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra los resultados para partido (opciones: Todos juntos, cada partido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de los resultados por partido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra los resultados para izquierda derecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de los resultados por izquierda derecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra los resultados para Nolan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de los resultados para Nolan\n",
    "\n",
    "Acá también se concluye que no esta mostrando nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4. Evaluación\n",
    "***\n",
    "Acá se evalua si PCA encara (para saber si el corpus es el que esta mal o es PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra valores propios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de que hay un PC muy grande y eso significa que no encara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra ratio de varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de que al reducir a 2 dimensiones se perdio mas del 60% de la info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra matriz de covarianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se analiza la matriz de covarianza y como impacta la relacion entre las preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Análisis - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1. Proceso\n",
    "***\n",
    "Acá mencionar que cosas corrimos\n",
    "\n",
    "#### 4.5.2. Resultados\n",
    "***\n",
    "Acá mostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que corre Kmeans y muestra grafica de torta de clusterings para k=2,3,5,10, mostrando Silhouette de cada uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de cual fue mejor silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3. Análisis cualitativo\n",
    "***\n",
    "Se hacen observaciones e interpretaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra para k=2,3,5,10 como se separan los candidatos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de que no se observa patrones en la separacion de candidatos pero que hay que ver con grupos mas grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra para k=11 como se separan los partidos y muestra el Silhouette y calcula el ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se analiza esto de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra para k=3 como se separan los espectros izquierda derecha y muestra el Silhouette y calcula el ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se analiza esto de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra para k=5 como se separan los candidatos y muestra el Silhouette y calcula el ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se analiza esto de arriba\n",
    "\n",
    "Se concluye cual es la mejor cantidad de clusters en base al silhouette y el ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Comparación\n",
    "***\n",
    "\n",
    "Acá se comparan las observaciones e interpretaciones surgidas en ambos enfoques, con el fin de ver si tienen lineamientos similares o se contraponen en algo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusiones\n",
    "***\n",
    "#### 5.1. Respecto a los Datos\n",
    "***\n",
    "Acá hablamos sobre el dataset, pudiendo mencionar cosas como:\n",
    "- La elección de atributos, probablemente desacertada\n",
    "- La representación de los datos y su relación con los candidatos a votar, probablemente sesgada\n",
    "\n",
    "#### 5.2. Respecto a los Enfoques\n",
    "***\n",
    "Acá hablamos sobre los enfoques (pca y kmeans), pudiendo mencionar cosas como:\n",
    "- Por que pca no fue util (demasiada reduccion de dimensionalidad, los nuevos atributos no agrupan bien las preguntas, etc)\n",
    "- Por que kmeans fue mas util pero tampoco separo del todo (depende de la cantidad de clusters, etc)\n",
    "\n",
    "#### 5.3. Respecto a los Resultados\n",
    "***\n",
    "Acá hablamos sobre los resultados de ambos enfoques (pca y kmeans), pudiendo mencionar cosas como:\n",
    "- Información que pudimos sacar gracias a algun enfoque o la mezcla de los dos (la separacion de partidos politicos, otros posibles espectros politicos)\n",
    "\n",
    "#### 5.4. Posibles mejoras\n",
    "***\n",
    "Para cerrar, se adjunta una lista de mejoras consideradas a la experimentación actual:\n",
    "- Como mejorar general (cambiar atributos)\n",
    "- Como mejorar pca (elegir otro especto, reducir la dimensionalidad a un valor optimo)\n",
    "- Como mejorar kmeans (probar mas clusters, idk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Referencias\n",
    "***\n",
    "[1] Hola\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
