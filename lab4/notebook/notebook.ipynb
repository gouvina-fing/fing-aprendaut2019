{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega 4 - Análisis de datos (PCA, K-Means)\n",
    "\n",
    "### Grupo 07:\n",
    "     - Renzo Gambone C.I. 5.155.486-4\n",
    "     - Germán Ouviña C.I. 4.823.566-1\n",
    "     - Leandro Rodríguez C.I 4.691.736-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\n",
    "***\n",
    "### 1.1. Objetivo\n",
    "***\n",
    "El objetivo de esta tarea fue analizar bajo distintos enfoques el corpus _aquienvoto.uy_. Más específicamente, se tuvieron en cuenta los siguientes lineamientos:\n",
    "- Implementar un algoritmo de reducción de dimensionalidad (algoritmo PCA) para visualizar los datos.\n",
    "- Implementar un modelo de aprendizaje no supervisado (algoritmo K-means) para agrupar los datos.\n",
    "- Analizar los resultados intermedios generados en cada enfoque, buscando patrones y extrayendo información de utilidad en el proceso.\n",
    "- Analizar los resultados finales generados en cada enfoque, contestando a las preguntas planteadas en el práctico y a otras surgidas de las observaciones del corups.\n",
    "- Enfocar dichos análisis desde un punto de vista cualitativo, teniendo en cuenta métricas cuantitativas con el fin de orientar dicho análisis.\n",
    "\n",
    "### 1.2. Entrega\n",
    "***\n",
    "La entrega de esta tarea consta de dos grandes componentes:\n",
    "- **Informe** en formato de Jupyter Notebook (este informe).\n",
    "- **Programa** que permite analizar el conjunto de datos utilizando **PCA** o **KMeans**, generar distintas gráficas, evaluar el desempeño de los algoritmos utilizados, entre otras utilidades.\n",
    "\n",
    "El objetivo del **informe** es centralizar la información relativa a los distintos métodos de análisis empleados, así como los resultados obtenidos y las conclusiones generadas. En esta ocasión, es posible ejecutar el código principal completamente desde el informe, agregandose las funciones necesarias en las secciones correspondientes (ver sección 1.3. para una descripción detallada sobre el código encontrado en cada sección).\n",
    "\n",
    "Por otra parte, el **programa** ofrece una interfaz en consola que permite la ejecución de ambos algoritmos acorde a distintas configuraciones paramétricas, así como generar todos los datos mostrados en el informe. Si bien todas estas herramientas fueron pensadas para uso del grupo, en el archivo *README.md* se adjunta una sencilla guía de como utilizarlas.\n",
    "\n",
    "A diferencia de las anteriores entregas, el **programa** y su interfaz se agregó como una herramienta puramente auxiliar. Como se mencionó anteriormente, es posible ejecutar el código principal en el **informe**. De todas formas, con el fin de facilitar la lectura, se redujo el uso de código dentro del informe lo máximo posible. Para un mayor entendimiento de los algoritmos empleados, referir a los archivos de código fuente.\n",
    "\n",
    "### 1.3. Estructura\n",
    "***\n",
    "Dado que en esta ocasión el objetivo principal está en el **análisis** y no en la **construcción** de uno o varios modelos, tanto la estructura como las tareas realizadas difieren bastante con respecto a las anteriores entregas. A continuación se adjunta una breve introducción del contenido de cada sección:\n",
    "\n",
    "- En la **Sección 2 (Enfoque)** se define y contrasta los dos grandes enfoques utilizados a lo largo de la tarea (reducción de dimensionalidad y aprendizaje no supervisado), manteniéndose dicha dualidad en el resto de secciones.\n",
    "- En la **Sección 3 (Diseño)** se tratan los aspectos matemáticos y técnicos de cada enfoque, definiendo los algoritmos empleados, formas de evaluación generales, etc.\n",
    "- En la **Sección 4 (Experimentación)** se define la metodología empleada a la hora de experimentar, así como el proceso de generación de resultados, los resultados en sí y el análisis posterior de los mismos. También se desarrolla una sección con información generada durante la investigación previa realizada en relación al marco teórico del corpus, justificando la toma de decisiones de los distintos experimentos realizados. \n",
    "- En la **Sección 5 (Conclusiones)** se adjuntan las conclusiones generales obtenidas en el desarrollo de la tarea, organizandose las mismas según distintas subsecciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enfoque\n",
    "***\n",
    "Como se mencionó en la sección anterior, el objetivo principal de la tarea es estudiar el corpus de _aquienvoto.uy_. El interés general radica en analizar la relación entre los potenciales votantes, buscando patrones en sus respuestas y enfocandose en el análisis cualitativo, es decir, priorizar las observaciones e interpretaciones de los datos.\n",
    "\n",
    "Existen mútliples estrategias posibles para encarar dicho problema, por lo que a continuación se definen los dos enfoques utilizados:\n",
    "\n",
    "### 2.1. Enfoque 1 - Reducción de Dimensionalidad\n",
    "***\n",
    "Por un lado, es posible analizar los datos sin utilizar ningún modelo de aprendizaje automático. Dado que el corpus cuenta con **26 atributos**, el primer enfoque se centra en la **reducción de dimensionalidad** del corpus en sí, buscando reducir la cantidad de dimensiones para poder visualizar los datos y buscar una relación entre votantes graficamente.\n",
    "\n",
    "Dicho enfoque puede presentar varios problemas, ya que para poder graficar los datos es necesario pasar de **26 dimensiones** a **2 dimensiones**, siendo factible una pérdida importante de información en el proceso. De hecho, dado un corpus específico, dependiendo de la técnica de reducción de dimensionalidad empleada y de la relación de los datos en sí, es posible determinar que tanto impactará la reducción de dimensiones en la integridad del corpus.\n",
    "\n",
    "De todas formas, si el corpus cuenta con una estructura \"resistente\" a la reducción de dimensiones y el algoritmo utilizado es compatible con dicha estructura, sería posible observar o descartar patrones entre los votantes.\n",
    "\n",
    "Tomando en cuenta lo anterior, existen múltiples técnicas de reducción de dimensionalidad. En el contexto de esta tarea se utilizará **PCA (Principal Component Analysis)**. En la sección 3.1 se especifican los aspectos técnicos de dicho algoritmo y en la sección 4.4 los resultados de la experimentación y análisis empleando dicha técnica.\n",
    "\n",
    "### 2.2. Enfoque 2 - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "Por otra parte, es posible analizar los datos utilizando modelos de aprendizaje automático, más concretamente modelos que implementen técnicas de **clustering** o **agrupamiento**, siendo está una forma de **aprendizaje no supervisado**.\n",
    "\n",
    "Dicho enfoque genera distintos grupos de votantes, basandose en su distribución en un espacio de **26 dimensiones**. Dependiendo de la naturaleza del corpus, esto puede generar problemas. Al igual que en el anterior enfoque, es posible determinar la cantidad óptima de grupos a generar (ya que a priori, esta es desconocida).\n",
    "\n",
    "Tomando en cuenta lo anterior, existen múltiples técnicas de clustering. En el contexto de esta tarea se utilizará **K-Means**. En la sección 3.2 se especifican los aspectos técnicos de dicho algoritmo y en la sección 4.5 los resultados de la experimentación y análisis empleando dicha técnica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Diseño\n",
    "***\n",
    "En esta sección se detallan las características del diseño utilizado para construir las herramientas de análisis para cada enfoque, profundizandose las estrategias y algoritmos empleados, así como otros puntos, como el procesamiento previo al análisis y/o entrenamiento y la evaluación posterior al mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Enfoque 1 - Reducción de Dimensionalidad\n",
    "***\n",
    "\n",
    "#### 3.1.1. Algoritmo\n",
    "***\n",
    "Como se mencionó en la sección anterior, el algoritmo empleado para reducir las dimensiones del corpus fue **PCA**. Como su nombre indica, el algoritmo hace uso de los **componentes principales** de una matriz. Se define como componente principal de una matriz al mayor **valor propio** de la misma.\n",
    "\n",
    "En términos generales, el algoritmo consiste en calcular $k$ componentes principales de cierta matriz, formar otra matriz con vectores propios correspondientes a cada componente principal y transformar el subespacio del corpus multiplicando cada vector del mismo por la matriz de vectores propios. Es importante destacar que $k$ es la cantidad de dimensiones que se quiere mantener, teniendo que ser entero positivo menor a la cantidad de dimensiones actual.\n",
    "\n",
    "Dado que existen múltiples métodos para implementar **PCA**, la matriz que se obtiene y por ende los componentes principales de la misma dependen del método. No obstante, los resultados son ortogonales al método, es decir, la reducción de dimensionalidad obtenida es independiente al método utilizado.\n",
    "\n",
    "Teniendo esto en cuenta, con el objetivo de validar métodos se implementaron dos formas distintas de **PCA**:\n",
    "\n",
    "##### 3.1.1.1. Matriz de Covarianza\n",
    "***\n",
    "\n",
    "Sea un vector de atributos $A = [A_1, ..., A_n]$ y sea $\\mu_i$ la media del atributo $A_i$, se define la **covarianza** entre dos atributos $A_i, A_j$ como la varianza conjunta de ambos atributos, utilizando la siguiente expresión:\n",
    "\n",
    "$$ Cov(A_i, A_j) = E((A_i - \\mu_i)(A_j - \\mu_j)) $$\n",
    "\n",
    "En cierta forma, la **covarianza** entre $A_i$ y $A_j$ representa la relación entre la varianza de ambos atributos por separado. Una **covarianza positiva** indica que ambos atributos crecen de forma _directamente proporcional_, mientras que una **covarianza negativa** indica que ambos atributos crecen de forma _inversamente proporcional_.\n",
    "\n",
    "De esta forma, la **matriz de covarianza** $M$ de un conjunto de datos $D$ con un vector de atributos $A$ se define como la covarianza de cada posible par de atributos de $A$, utilizando la siguiente expresión:\n",
    "\n",
    "$$M(D,A) = (Cov(A_i,A_j))_{ij} : 1 \\leq i, j \\leq |A| $$\n",
    "\n",
    "Como se mencionó anteriormente, **PCA** utiliza los componentes principales de una matriz para reducir la dimensión del subespacio que el corpus representa. En este método, la matriz empleada es nada más y nada menos que la **matriz de covarianza** para el corpus $D$ y sus atributos $A$.\n",
    "\n",
    "Teniendo esto en cuenta, el algoritmo implementado consiste en los siguientes pasos:\n",
    "1. Sustraer la media $\\mu_i$ para cada atributo $A_i$ del conjunto de datos (esto transforma al corpus en un conjunto de datos con media 0 para cada atributo).\n",
    "2. Calcular la **matriz de covarianza** $M(D,A)$.\n",
    "3. Calcular los **valores propios** de $M(D,A)$ y obtener un **vector propio** para cada valor.\n",
    "4. Elegir los $k$ **componentes principales** de $M(D,A)$ y formar la matriz $W$ con los $k$ vectores propios correspondientes a los valores propios elegidos (y en el orden adecuado).\n",
    "5. Sea $D$ la matriz que representa al corpus original, se calcula $T = DW$, teniendo esta $k$ dimensiones.\n",
    "\n",
    "\n",
    "##### 3.1.1.2. Descomposición SVD\n",
    "***\n",
    "\n",
    "Otro potencial método para obtener una matriz de la cual extraer los componentes principales es la **descomposición SVD** o **descomposición en valores singulares**. \n",
    "\n",
    "Sea $M \\in R_{m \\times n}$ una matriz, su descomposición SVD se define como una factorización del tipo $M = USV$ donde $U,S,V$ son matrices que cumplen las siguientes propiedades:\n",
    "- La matriz $U \\in R_{m \\times m}$ está formada por vectores propios de $M . M^t$, siendo $U$ ortonormal.\n",
    "- La matriz $V \\in R_{n \\times n}$ está formada por vectores propios de $M^t . M$, siendo $V$ ortonormal.\n",
    "- La matriz $S \\in R_{m \\times n}$ está formada por los **valores singulares** de $M$ en su diagonal principal ordenados de mayor a menor.\n",
    "\n",
    "Cabe destacar que los **valores singulares** de una matriz son las raíces cuadradas de sus **valores propios** por lo que el mayor valor singular de una matriz es la raíz cuadrada del componente principal de dicha matriz.\n",
    "\n",
    "El método implementado se sirvió de obtener los valores propios de la matriz de covarianza, utilizando luego aritmética de matrices para obtener una matriz $T$ que represente al corpus $D$ con dimensionalidad reducida.\n",
    "\n",
    "Dado que el método con **SVD** no aporta más que una validación a la hora de comparar con el método anterior, no se entrará en detalles de su implementación. Para ver dicha implementación, referir al código fuente del módulo _pca.py_.\n",
    "\n",
    "#### 3.1.2. Evaluación\n",
    "***\n",
    "\n",
    "Los algoritmos de reducción de dimensionalidad tienen distinto rendimiento dependiendo de su forma de operar y del conjunto de datos sobre el que trabajen. En cualquier caso, cuanto más se reducen las dimensiones de un corpus, más información se pierde.\n",
    "\n",
    "Normalmente, al momento de reducir las dimensiones de un conjunto de datos utilizando **PCA**, surge la necesidad de decidir cuántas componentes se van a considerar para que no se pierda el significado que contienen los datos en sí. Para esto se suele calcular una métrica denominada **ratio de varianza explicada**, la cual permite determinar en términos generales la pérdida de información provocada al reducir las dimensiones para cada posible $k$ que se tome. De esta forma, dependiendo de donde se ponga el límite, el menor $k$ que supere ese límite sería la cantidad \"ideal\" de componentes a tomar.\n",
    "\n",
    "Para calcular el **ratio de varianza explicada** se procesan los valores propios, calculándose la suma acumulativa para cada cantidad de componentes consideradas en cada caso. De esta forma, para cierto $k$, se obtiene que esta métrica representa la suma de los $k$ componentes principales sobre la suma de todos los valores propios. Cuanto mayor es este valor, mejor es esa elección de $k$. Lógicamente, cuanto mayor es $k$, mayor es el ratio de varianza explicada (de hecho, es creciente).\n",
    "\n",
    "En este contexto en particular, se sabe que el interés de utilizar **PCA** es reducir a **2 dimensiones**, fijando $k = 2$. No obstante, utilizando el **ratio de varianza explicada**, se puede conocer que tanta información se está perdiendo. En la sección de experimentación 4.4 se muestran los resultados de la evaluación de **PCA** y sus implicancias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Enfoque 2 - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "\n",
    "#### 3.2.1. Modelo\n",
    "***\n",
    "Las técnicas de **aprendizaje no supervisado** difieren sustancialmente de las técnicas implementadas en anteriores entregas, las cuales son del grupo de técnicas de **aprendizaje supervisado**. En esencia, el aprendizaje supervisado utiliza para entrenar un conjunto de datos con clasificaciones conocidas, mientras que el aprendizaje no supervisado, en lugar de vincular elementos del conjunto con su clasificación, se concentra en observar patrones e irregularidades en la distribución de los elementos en el espacio comprendido por sus atributos.\n",
    "\n",
    "Dentro de las técnicas de aprendizaje no supervisado, existe una que se denomina **clustering**, la cual tiene como objetivo agrupar los elementos del conjunto de datos en subconjuntos disjuntos denominados **clusters**, los cuales agrupan elementos similares de alguna forma en cuanto a sus atributos, siguiendo distintas nociones dependiendo del modelo implementado.\n",
    "\n",
    "Cabe mencionar que los resultados obtenidos no necesariamente reflejan conclusiones que sean de valor para un clasificador, sino que los mismos están orientados netamente hacia la distribución de los elementos en el conjunto de atributos. Observar como los elementos están distribuidos no es trivial para un corpus con alta dimensionalidad, pero permite destacar anomalías en el conjunto y agrupar ejemplos \"similares\". Por otra parte, es posible que la agrupación en clusters esté vinculada con la clasificación de los elementos de un cluster, sin embargo esta presunción no puede realizarse sin la experimentación adecuada.\n",
    "\n",
    "Teniendo en cuenta las generalidades anteriormente descritas, en el contexto de esta tarea se implementó el modelo de **clustering** denominado **K-Means Clustering** (a partir de ahora denominado **K-Means**), el cual representa el conjunto de datos como un espacio de **$n$ dimensiones** (siendo $n$ la cantidad de atributos) e itera sobre el mismo, buscando determinar los anteriormente mencionados **clusters**.\n",
    "\n",
    "Se definen más formalmente las siguientes nociones:\n",
    "* $D =$ Conjunto de entrenamiento.\n",
    "* $C_D =$ Conjunto de posibles clasificaciones para $d \\in D$.\n",
    "* $KMeans_{(D,k)} =$ Agrupación del conjunto $D$ en $k$ clusters, los cuales son representados por sus **centroides**, los cuales son coordenadas en el espacio de los atributos que determinan la posición espacial de cada cluster.\n",
    "* Para un ejemplo $d \\in D$, es posible determinar a que cluster pertenece asociandolo al cluster cuyo centroide esté más próximo a el mismo.\n",
    "\n",
    "Los detalles sobre el algoritmo de entrenamiento y de agrupación de nuevos ejemplos se expanden en la siguiente sección.\n",
    "\n",
    "#### 3.2.2. Algoritmo\n",
    "***\n",
    "\n",
    "##### 3.2.2.1. Algoritmo de entrenamiento\n",
    "***\n",
    "El algoritmo **K-Means** implementado sigue los lineamientos del visto en el teórico, por lo que no es necesario aclarar ningúna particularidad de su implementación, más que el hecho de que el umbral de variación entre los centroides obtenidos en una iteración y la siguiente debe ser de 0.0001 o menor para que el algoritmo considere haber terminado.\n",
    "\n",
    "##### 3.2.2.2. Algoritmo de agrupación\n",
    "***\n",
    "Con el objetivo de separar conceptualmente de la clasificación vista en modelos de aprendizaje supervisado, se le denomina **agrupación** a determinar el cluster al que debería pertenecer un nuevo ejemplo que no se encontraba en el conjunto de datos original.\n",
    "\n",
    "Dada la naturaleza de **K-Means**, el algoritmo de agrupación es posible y ligero en terminos de computo. Consiste en simplemente calcular la distancia euclídea del ejemplo a agrupar para cada uno de los centroides de cada cluster. Aquel centroide que se encuentre más cerca del ejemplo, representará al cluster al que dicho ejemplo pertenece.\n",
    "\n",
    "Es crucial mencionar que nuevos ejemplos podría afectar a los clusters generados originalmente, cambiando la distribución de ejemplos e incluso la ubicación de los centroides. Por ende, para ciertos escenarios es incorrecto utilizar este enfoque de agrupación de nuevos ejemplos y debe correrse el algoritmo de entrenamiento otra vez. Dado que la implementación para esta tarea no tiene en cuenta la agrupación de nuevos ejemplos en ningún punto, se consideró un enfoque posible. \n",
    "\n",
    "#### 3.2.4. Evaluación\n",
    "***\n",
    "Debido a que el aprendizaje no supervisado no apela a generar un clasificador y validarlo con un subconjunto de prueba, no es posible evaluar su rendimiento utilizando las métricas de tareas anteriores. De hecho, a priori no existe una clasificación \"correcta\".\n",
    "\n",
    "Teniendo esto en cuenta, se evaluaron los potenciales problemas del modelo **K-Means** y se planteó el uso de distintas métricas para determinar que clusters agrupan mejor a los datos. A continuación se especifican dichos problemas:\n",
    "\n",
    "##### 3.2.4.1. Problemas del modelo\n",
    "***\n",
    "\n",
    "**K-Means** tiene dos problemas principales:\n",
    "\n",
    "* La función de costo a minimizar no es convexa, y por ende, tiene óptimos locales. A su vez el algoritmo **K-Means** tiene solamente componentes de *explotación* y no de *exploración*, por lo que es propenso a quedarse atrapado en el primer óptimo local que encuentre.\n",
    "\n",
    "* La elección del número $k$ no es trivial para cada conjunto de datos. Un valor de $k$ bajo será más eficiente computacionalmente y tendrá una función de costo más fácil de optimizar, pero las agrupaciones en clusters no necesariamente reflejarán una clara distribución de los elementos en el conjunto. Por otra parte, un valor de $k$ alto puede tener una mayor cantidad de óptimos locales y puede subdividir el conjunto en más clusters de lo que realmente tiene sentido para la distribución del conjunto.\n",
    "\n",
    "Sabiendo esto, en la experimentación se decidió implementar ciertas estrategias con el objetivo de palear la influencia de estos problemas, siendo las mismas:\n",
    "\n",
    "* Realizar $n$ ejecuciones independientes del algoritmo (siendo $n$ un parámetro de entrada), manteniendo los resultados de mejor calidad, acorde a cierta métrica (la cual se define en la siguiente subsección). De esta forma se busca dar un componente de *exploración* al algoritmo.\n",
    "\n",
    "* Realizar ejecuciones para distintos valores de $k$, estudiando su comportamiento en relación al cambio de $k$.\n",
    "\n",
    "En la sección de experimentación 4.5 se profundiza sobre los enfoques tomados y los resultados obtenidos gracias a ello.\n",
    "\n",
    "##### 3.2.4.2. Métricas empleadas\n",
    "***\n",
    "\n",
    "Para cuantificar la calidad del agrupamiento en clusters del algoritmo **K-Means** se emplearon dos métricas distintas que ofrecen distintas nociones sobre la calidad de los clusters elegidos. Las implementaciones de dichas métricas son las brindadas por la biblioteca *sklearn* de Python.\n",
    "\n",
    "Es importante mencionar que, de cierta forma, estas métricas atentan contra la idea del aprendizaje no supervisado, ya que se utilizan clasificaciones conocidas para evaluar la calidad de los clusters. Dichas clasificaciones no siempre están disponibles y a veces ni siquiera existen. Sin embargo, en este contexto se cuenta con esos datos, por lo cual se utilizarán para encontrar patrones.\n",
    "\n",
    "Las métricas son las siguientes:\n",
    "\n",
    "###### 3.2.4.2.1. Silhouette\n",
    "***\n",
    "El **coeficiente de Silhouette** evalúa los resultados del modelo observando la densidad de cada cluster según la clasificación original de cada elemento. Un coeficiente alto se relaciona con que los resultados tienen clusters \"bien definidos\".\n",
    "\n",
    "Dados los resultados:\n",
    "* **a:** La distancia promedio entre una muestra y todos los otros puntos de la misma clase.\n",
    "* **b:** La distancia promedio entre una muestra y todos los otros puntos en el cluster más proximo.\n",
    "\n",
    "Se define el **coeficiente de Silhouette** como: $$s = \\frac{b-a}{max(a,b)}$$\n",
    "\n",
    "Para un conjunto de muestras, se toma el promedio general del **coeficiente de Silhouette** para cada muestra.\n",
    "\n",
    "###### 3.2.4.2.2. Adjusted Random Index (ARI)\n",
    "***\n",
    "La métrica **ARI** se encuentra comprendida en el intervalo $[-1..1]$, y dadas dos listas con valores \"reales\" y \"predecidos\" mide la similaridad entre los elementos de las mismas, ignorando posibles permutaciones. En esencia, esta métrica es útil para comprobar si efectivamente cada cluster está vinculado con las clasificaciones originales del conjunto o con otros posibles agrupamientos conocidos.\n",
    "\n",
    "A diferencia del *coeficiente de Silhouette*, esta métrica sólo tiene sentido para determinados valores de $k$ (valores a los cuales se pueda asociar una clasificación), dado que no evalúa según la densidad de cada cluster, sino según como el conjunto entero fue agrupado en clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experimentación\n",
    "***\n",
    "En esta sección se detallan los aspectos relacionados a las distintas pruebas realizadas, tratando tanto los lineamientos previos como los análisis realizados sobre los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Metodología\n",
    "***\n",
    "\n",
    "Dado el objetivo de buscar patrones en el corpus, se utilizó una metodología de experimentación particular, dividiendola en las siguientes etapas:\n",
    "\n",
    "1. **Investigación previa:** Al tratarse de un corpus con un contexto específico (posibles votaciones en elecciones nacionales), fue necesario realizar una investigación genérica sobre dicho contexto para poder evaluarlo lo mejor posible. En resumen, se investigó sobre generalidades de la política partidaria, análisis estadísticos en poblaciones de votantes, variables indicadoras, entre otros. En la sección 4.2 se profundiza la información recabada y los sesgos introducidos por la interpretación de la misma.\n",
    "<br><br>\n",
    "2. **Análisis del corpus:** Habiendo realizado el sondeo de información básica sobre el tema, se analizó la estructura del corpus, especulando sobre su correctitud. En la sección 4.3 se estudian las posibles ventajas y desventajas de esta estructura.\n",
    "<br><br>\n",
    "3. **Generación de resultados:** Una vez realizada la investigación y el análisis preliminar, se ejecutaron distintas configuraciones de ambos enfoques con el fin de observar los datos resultantes. En la sección 4.4 se expanden las elecciones de configuraciones utilizadas, la justificación de dichas elecciones, los resultados más importantes y se adjuntan los scripts que fueron implementados.\n",
    "<br><br>\n",
    "4. **Análisis de enfoques:** Teniendo los resultados generados, se realizó un estudio de los mismos con un enfoque de análisis cualitativo, utilizando los conocimientos generados en las etapas previas. Se analizaron métricas descriptivas de ambos algoritmos, también realizandose observaciones e interpretaciones de cada resultado.En la sección 4.5 se expanden estos puntos y se adjuntan los scripts necesarios para evaluar el desempeño del algoritmo utilizado y la calidad de los resultados analizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Marco Teórico\n",
    "***\n",
    "\n",
    "Dada la naturaleza del corpus, fue necesario emplear cierta cantidad de tiempo en realizar una investigación previa, con el objetivo de contextualizar el análisis. Por una parte, fue necesario investigar sobre generalidades de **política**, como el concepto de **partido político** y distintas formas de clasificar posturas dentro de diversos **espectros políticos**. Una vez se hubieron bajado a tierra dichos conceptos, fue necesario aplicarlos al contexto nacional y contemporáneo, investigando las propuestas de cada candidato.\n",
    "\n",
    "Dado que no es el objetivo de esta tarea, no se entrará en detalles de los conceptos teóricos estudiados. En la sección 6 (Referencias) se adjuntan los textos utilizados en la investigación.\n",
    "\n",
    "De esta forma, viendo a cada ejemplo del corpus como un **votante**, se determinaron 4 posibles agrupamientos de los mismos, con el fin de estudiar su distribución en el corpus y su relación con los atributos. De estos 4 grupos, los 2 primeros son **triviales** (no existe dos formas de interpretarlos), mientras que los últimos 2 son **sesgados** (se pueden interpretar de varias formas). Dichos grupos se generaron en base a los siguientes lineamientos:\n",
    "\n",
    "1. **Candidatos:**\n",
    "    \n",
    "    El agrupamiento predefinido, donde existe un grupo por candidato y cada grupo contiene a los votantes de dicho candidato. Existen **24 candidatos**.\n",
    "\n",
    "\n",
    "2. **Partidos:** \n",
    "    \n",
    "    Otro agrupamiento trivial, donde existe un grupo por partido político y cada grupo contiene a los votantes de aquellos candidatos que pertenecen al partido. Dado que cada candidato pertenece a un único partido, no existe ambiguedad en la asignación de partidos a candidatos. Existen **11 partidos**.\n",
    "\n",
    "\n",
    "3. **Espectro unidimensional:**\n",
    "\n",
    "    El primero de los agrupamientos sesgados, denominándose así ya que la asignación de candidatos no es trivial ni única, pudiendo intepretarse de varias formas. Este agrupamiento se fundamenta en los principios denominados **izquierda** y **derecha**, originados durante la revolución francesa y mantenidos a lo largo del tiempo. Si bien en ciertos ámbitos de la política se pueden considerar obsoletos, ambos grupos representan dos \"extremos\" de un espectro político unidimensional. \n",
    "\n",
    "    Originalmente, se consideraba de **derecha** a aquellos individuos que defendían el status qúo, mientras que los individuos de **izquierda** buscaban el cambio, entendiendo al status qúo como injusto y retrógrado. Con el paso del tiempo y el cambio rotundo en la organización de muchas sociedades del mundo, se empezó a considerar a la **derecha** como aquella ideología que defendía la existencia de las clases sociales y la desigualdad social, mientras que la **izquierda** es la ideología que se opone a dicha noción. Si bien los intereses que ambos grupos defienden varían dependiendo del contexto histórico y regional, la utilización de ambos términos para agrupar personas según esta ideología política prevalece, ya que a grandes rasgos captan la esencia de polos opuestos.\n",
    "\n",
    "    Teniendo esto en cuenta, es importante marcar que si bien se oponen, existe un **espectro** de por medio, denominandose **extrema izquierda** y **extrema derecha** respectivamente a aquellas ideologías que no toman ninguna característica de la opuesta. Una postura que combina características de ambos extremos \"en partes iguales\" se denomina **central**, mientras que las tendencias del centro a cada extremo son denominadas **centroizquierda** y **centroderecha** respectivamente.\n",
    "    \n",
    "    De esta forma, y luego de investigar las posturas de cada candidato y partido ante estas clasificaciones, se determinaron **3 grupos**: izquierda, centro y derecha. A su vez, se clasificó a cada candidato dentro de cada uno de los grupos. Dicha clasificación puede ser incorrecta, por lo que los resultados se tomarán con pinzas.\n",
    "\n",
    "\n",
    "4. **Espectro bidimensional:**\n",
    "\n",
    "    Este agrupamiento sesgado también se fundamenta en los principios denominados **izquierda** y **derecha**, pero complejiza el espectro agregando una segunda dimensión y modificando la idea de la primera. Dichas dimensiones se denominan **libertad personal** y **libertad económica**, la primera haciendo referencia a que aquellas políticas sociales que dan libertad a los individuos en su vida diaria implican una mayor libertad personal, mientras que la segunda hace referencia a que aquellas políticas económicas que dan libertad a individuos o empresas para operar con poco control del Estado implican una mayor libertad económica.\n",
    "       \n",
    "    Ambas nociones fueron acuñadas por _David Nolan_, quien inventó la conocida **gráfica de Nolan** que utiliza ambas nociones para clasificar en un espectro bidimensional a candidatos o partidos según la relación de sus políticas con los conceptos de liberad personal y económica.\n",
    "\n",
    "    <div style=\"display: inline-block; width: 99%; text-align: center; margin-top: 16px;\">\n",
    "        <img src=\"img/nolan.png\" />\n",
    "        <label style=\"margin-top: 16px; font-size: 16px; font-family: monospace;\"> Figura 4.2 - Gráfica de Nolan</label>\n",
    "    </div>\n",
    "    \n",
    "    En la gráfica se definen los 4 extremos como **progresismo**, **liberalismo**, **conservadurismo**, **totalitarismo**. Tomando el espectro unidimensional, el progresismo es asociado con la izquierda (libertad personal alta pero control económico por parte del Estado), mientras que el conservadurismo es asociado con la derecha (privatización de empresas y poco control del Estado en materias económicas, pero fuerte imposición de normas sociales). Por otra parte, el totalitarismo es asociado con el control total por parte del Estado en la población, mientras que el liberalismo sitúa al Estado como un mero actor con mucha menos influencia.\n",
    "    \n",
    "    De esta forma, y luego de investigar las posturas de cada candidato y partido ante estas clasificaciones, se determinaron **5 grupos**: progresismo, liberalismo, conservadurismo, totalitarismo y centro. A su vez, se clasificó a cada candidato dentro de cada uno de los grupos. Dicha clasificación fue sumamente tendenciosa dada la falta de información y lo complejo de establecer un límite en una clasificación espectral, por lo que es muy potencialmente errónea. No obstante, se decidió mantenerla con el fin de identificar algún posible patrón.\n",
    "\n",
    "Es importante volver a destacar que dichas separaciones se encuentran sumamente sesgadas, utilizándose simplemente para obtener una noción de como se distribuye la información en el corpus y en los clusters generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Análisis - Corpus\n",
    "***\n",
    "\n",
    "Habiendo generado una bae para poder analizar el corpus y determinar su correctitud cualitativamente, se realizaron las siguientes observaciones sobre el **conjunto de atributos**:\n",
    "- Hay 3 grupos grandes de preguntas: **Economía**, **Seguridad** y **Sociedad**.\n",
    "- Hay 26 preguntas en total, las cuales perteneces a alguno de los 3 grupos anteriores.\n",
    "- Hay 5 posibles puntajes, del 1 al 5, para cada pregunta. Cuánto mayor puntaje, mayor acuerdo se expresa con la pregunta.\n",
    "\n",
    "Tomando en cuenta dichas observaciones, se realizaron ciertas interpretaciones:\n",
    "- Las categorías **economía** y **sociedad** agrupan dos de los grandes conceptos tratados en la **gráfica de Nolan**, sin embargo, la noción de **seguridad** no entra en dicho apartado. Bajo cierta interpretación, podría considerarse parte de lo que es sociedad. Dado que es un tema recurrente en los medios uruguayos desde hace algunos años, puede tener sentido que exista un conjunto apartado de preguntas sobre este tema, mas también puede considerarse tendencioso y por ende puede estar agregando ruido.\n",
    "- Las preguntas tienen un carácter bastante general y asignan una representación númerica al acuerdo con cada una. Dada la complejidad de los conceptos que se manejan en algunas preguntas, es posible que dicha representación de respuestas también genere ruido.\n",
    "\n",
    "De todas formas, dado lo complejo que puede llegar a ser el análisis matemático de variables sociales, es entendible que se hayan elegido ciertas representaciones, teniendo en cuenta el posible alejamiento de la realidad.\n",
    "\n",
    "En lo que respecta al **conjunto de datos**, dicho análisis se deja para las siguientes secciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Análisis - Reducción de dimensionalidad\n",
    "***\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1. Proceso\n",
    "***\n",
    "Acá mencionar que cosas corrimos\n",
    "\n",
    "#### 4.4.2. Resultados\n",
    "***\n",
    "Acá mostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que corre PCA y muestra resultados generales (grafica sin diferenciacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3. Análisis cualitativo\n",
    "***\n",
    "Acá no se compara, se hacen observaciones e interpretaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra los resultados para partido (opciones: Todos juntos, cada partido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de los resultados por partido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra los resultados para izquierda derecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de los resultados por izquierda derecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra los resultados para Nolan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de los resultados para Nolan\n",
    "\n",
    "Acá también se concluye que no esta mostrando nada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4. Evaluación\n",
    "***\n",
    "Acá se evalua si PCA encara (para saber si el corpus es el que esta mal o es PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra valores propios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de que hay un PC muy grande y eso significa que no encara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra ratio de varianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de que al reducir a 2 dimensiones se perdio mas del 60% de la info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra matriz de covarianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se analiza la matriz de covarianza y como impacta la relacion entre las preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Análisis - Aprendizaje no Supervisado (Clustering)\n",
    "***\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1. Proceso\n",
    "***\n",
    "Acá mencionar que cosas corrimos\n",
    "\n",
    "#### 4.5.2. Resultados\n",
    "***\n",
    "Acá mostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que corre Kmeans y muestra grafica de torta de clusterings para k=2,3,5,10, mostrando Silhouette de cada uno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de cual fue mejor silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3. Análisis cualitativo\n",
    "***\n",
    "Se hacen observaciones e interpretaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra para k=2,3,5,10 como se separan los candidatos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se habla de que no se observa patrones en la separacion de candidatos pero que hay que ver con grupos mas grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra para k=11 como se separan los partidos y muestra el Silhouette y calcula el ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se analiza esto de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra para k=2,3 como se separan los espectros izquierda derecha y muestra el Silhouette y calcula el ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se analiza esto de arriba, viendo cuales son los candidatos de centro que estan en el cluster izquierda y cuales en el derecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acá script que muestra para k=5 como se separan los candidatos y muestra el Silhouette y calcula el ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá se analiza esto de arriba\n",
    "\n",
    "Se concluye cual es la mejor cantidad de clusters en base al silhouette y el ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusiones\n",
    "***\n",
    "#### 5.1. Respecto a los Datos\n",
    "***\n",
    "- La política es una ciencia social y por lo tanto, está lejos de ser exacta. Existen múltiples variables a tomar en cuenta al momento de hacer interpretaciones sobre cualquier conjunto de datos, siendo altamente probable la presunción de hipótesis incorrectas.\n",
    "- Definir un conjunto de preguntas tendenciosas puede sesgar la generación de grupos y la asignación de candidatos a votantes.\n",
    "- Definir respuestas numéricas ante preguntas de materias muy generales no sólo simplifica excesivamente un fenómeno tan complejo como la ideología política, sino que también asume que el sujeto en cuestión cuenta con los conocimientos necesarios para responder acorde a su ideología.\n",
    "\n",
    "#### 5.2. Respecto a los Resultados\n",
    "***\n",
    "- La reducción de dimensionalidad puede resultar útil en ciertos conjuntos de datos, pero en este caso se perdió demasiada información y por lo tanto fue imposible determinar algun patrón.\n",
    "- Por otra parte, el clustering demostró resultar de utilidad y separar en clusters con proporciones distintas de cada grupo político. No obstante, la separación estuvo lejos de ser 100% acertada. Este hecho puede ser un indicador de que el modelo no es lo suficientemente bueno para el conjunto de datos, o que el espectro de separación es más complejo de lo que se determinó\n",
    "- El separar en grupos conceptuales y asignar a cada candidato un grupo arbitrariamente, particularmente los del espectro político, puede haber influido negativamente en el análisis, ya que puede haber sesgado incorrectamente cierto grupo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Referencias\n",
    "***\n",
    "[1] **Teoría analítica de la política** - *Melvin J. Hinich y Michael C. Munger* (2003)\n",
    "\n",
    "[2] **Un centro vacío de candidatos: evaluando modelos espaciales para las elecciones presidenciales en Uruguay** - *Juan Andrés Moraes y Diego Luján (2016)*\n",
    "\n",
    "[3] **The Political Spectrum: A Bi-Dimensional Approach** - *Maurice Bryson y William McDill* (1968)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
